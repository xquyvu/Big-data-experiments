{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data from git\n",
    "The cell below will create a local copy from our Github repository into the local filesystem. This only needs to be done once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'City-Data-Science'...\n",
      "remote: Counting objects: 41, done.\u001b[K\n",
      "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
      "remote: Total 41 (delta 7), reused 21 (delta 1), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (41/41), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tweyde/City-Data-Science.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the local copy of the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science\n",
      "From https://github.com/tweyde/City-Data-Science\n",
      " * branch            HEAD       -> FETCH_HEAD\n",
      "Already up-to-date.\n",
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work\n"
     ]
    }
   ],
   "source": [
    "%cd City-Data-Science/\n",
    "!git pull https://github.com/tweyde/City-Data-Science.git\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Word preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house\n"
     ]
    }
   ],
   "source": [
    "def stripFinalS( word ):\n",
    "    if len(word) > 1:\n",
    "        if word[-1] == 's':\n",
    "            word = word[:-1]\n",
    "    else:\n",
    "        print('The string is empty')\n",
    "        \n",
    "    return word.lower()\n",
    "print(stripFinalS('houses')) # for testing, should return 'house'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 1218\n",
      "i: 1019\n",
      "and: 1019\n",
      "to: 834\n",
      "a: 815\n",
      "of: 733\n",
      "you: 610\n",
      "my: 516\n",
      "in: 464\n",
      "it: 442\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "import re\n",
    "\n",
    "linesRDD = sc.textFile(\"./City-Data-Science/library/hamlet.txt\") # read text as RDD\n",
    "wordsRDD = linesRDD.flatMap(lambda line: re.split('\\W+',line)) # split words, break lists\n",
    "wordsFilteredRDD = wordsRDD.filter(lambda word: len(word)>0)\n",
    "\n",
    "words1RDD = wordsFilteredRDD.map(lambda word: (stripFinalS(word),1)) # lower case, (w,1) pairs\n",
    "wordCountRDD = words1RDD.reduceByKey(add) # reduce and add up counts\n",
    "freqWordsRDD = wordCountRDD.filter(lambda x:  x[1] >= 5 ) # remove rare words\n",
    "output = freqWordsRDD.sortBy(lambda x: -x[1]).take(10) # collect 1o most frequent words\n",
    "for (word, count) in output: # iterate over (w,c) pairs\n",
    "    print(\"%s: %i\" % (word, count)) #  â€¦ and print\n",
    "# this sohuld print the stopwords with their "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Extracting word frequency vectors from text documents\n",
    "\n",
    "Now we start a new script, which reads in a whole directory with text files and extracts word frequency information.\n",
    "\n",
    "This involves some tuple restructuing and list transformation. It is important to use meaningful variable names. Also it is helpful to use pen and paper (or a text editor) to write down the structures that you are intending to create. Keep in mind the final goal of getting a list of words and their frequencies for each file, i.e. (filename,[(w,c), ... , (w,c)]). \n",
    "\n",
    "\n",
    "## 2a) Load the files\n",
    "Load all text files in the directory /data/student/bigdatastud/library on the server lewes using sc.wholeTextFiles <br>(see [http://spark.apache.org/docs/2.0.0/api/python/pyspark.html#pyspark.SparkContext.wholeTextFiles](http://spark.apache.org/docs/2.0.0/api/python/pyspark.html#pyspark.SparkContext.wholeTextFiles)). This will create an RDD with tuples of the structure (filepath,content), where content is the whole text from the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partitions:  2\n",
      "elements:  19\n"
     ]
    }
   ],
   "source": [
    "dirPath = \"./City-Data-Science/library/\"\n",
    "fw_RDD = sc.wholeTextFiles(\"./City-Data-Science/library/\")\n",
    "\n",
    "print(\"partitions: \", fw_RDD.getNumPartitions()) # on IBM DSX we have 2 executors by default with one partition each\n",
    "print(\"elements: \", fw_RDD.count())\n",
    "# this should print partitions:  2 and elements:  19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) Split the RDD elements using flatMap to make the (filename, word) tuples the key.\n",
    "\n",
    "For this define a function that takes a pair `(filename,content)` and output list of pairs `[(filename, word1), ...(filename, wordN)]`.\n",
    "\n",
    "Use list comprehensions (see http://www.pythonforbeginners.com/basics/list-comprehensions-in-python) to iterate through the word list in a for loop, and append the (filename,word) tuples to a new list.  \n",
    "\n",
    "Below is a template, you need to fill in the that starts with `<<<`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/prideandpredjudice.txt',\n",
       "  'The')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def splitFileWords(filenameContent): \n",
    "    f,c = filenameContent # unpack the input tuple  \n",
    "    fwLst = [] # the new list for (filename,word) tuples\n",
    "    wLst =  re.split('\\W+', c) # <<< now create a word list wLst be splitting c (the content)\n",
    "    for w in wLst: # iterate through the list\n",
    "         if len(w) > 0:\n",
    "            fwLst.append((f,w)) # <<< and append (f,w) to the fwList\n",
    "    return fwLst #return a list of (f,w) tuples \n",
    "    \n",
    "fw_RDD = fw_RDD.flatMap(splitFileWords)\n",
    "fw_RDD.take(1)\n",
    "\n",
    "# should print something similar to this:\n",
    "# [('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/prideandpredjudice.txt',\n",
    "# 'The'),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "Looking that the new elements, what might be problematic if we were using a large dataset and what could we do to prevent this from happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use filter to keep only the tuples with stopwords (remember, the words are now the 2nd element of the tuple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/tempest.txt',\n",
       "  'you'),\n",
       " ('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/tempest.txt',\n",
       "  'you'),\n",
       " ('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/tempest.txt',\n",
       "  'you')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwlst = ['the','a','in','of','on','at','for','by','I','you','me'] # stopword list\n",
    "fw_RDD2 = fw_RDD.filter(lambda x: x[1] in stopwlst) #<<< filter, keeping only stopwords as 2nd part of the tuples\n",
    "fw_RDD2.top(3) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2c) Count the words and reorganise the tuples to count: ((filename,word), count)\n",
    "\n",
    "Now you can package the elements into tuples with 1s and use reduceByKey(add) to get the counts of the words per filename, similar to last week and in task 1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/tempest.txt',\n",
       "   'you'),\n",
       "  260),\n",
       " (('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/tempest.txt',\n",
       "   'the'),\n",
       "  695),\n",
       " (('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/tempest.txt',\n",
       "   'on'),\n",
       "  85)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw_1_RDD = fw_RDD2.map(lambda x: (x,1))  #<<< change (f,w) to ((f,w),1)\n",
    "fw_c_RDD = fw_1_RDD.reduceByKey(add) #<<< count the words\n",
    "fw_c_RDD.top(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d) Creating and concatenating lists\n",
    "\n",
    "As a next step, map the `((filename,word),count)` eleemnts to `( filename, [ (word, count) ])` structure, i.e. rearange and wrap a list aournd the one tuple (just by writing squre backets). For this create a function `reGrpLst` to regroup and create a list. Check that the output has the intended structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/tempest.txt',\n",
       "  [('you', 260)]),\n",
       " ('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/tempest.txt',\n",
       "  [('the', 695)]),\n",
       " ('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/tempest.txt',\n",
       "  [('on', 85)])]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reGrpLst(fw_c): # we get a nested tuple\n",
    "    fw,c = fw_c # unpack the outer tuple\n",
    "    f,w = fw\n",
    "    return (f, [(w,c)]) # return (f,[(w,c)]) structure.\n",
    "\n",
    "f_wcL_RDD = fw_c_RDD.map(reGrpLst) \n",
    "f_wcL_RDD.top(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can concatenate the lists per filename using reduceByKey(). Write a lambda that cancatenates the lists per element.  Concatenation of lists in Python is done with '+', e.g.  `[1,2] + [3,4]` returns `[1,2,3,4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_wcL2_RDD = f_wcL_RDD.reduceByKey(lambda wc1,wc2: wc1+wc2 ) #<<< create [(w,c), ... ,(w,c)] lists per file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/hamlet.txt', [('in', 427), ('of', 685), ('at', 86), ('you', 544), ('I', 617), ('the', 1050), ('for', 202), ('on', 141), ('by', 124), ('a', 496), ('me', 236)])\n",
      "\n",
      "('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/merchant_of_venice.txt', [('at', 73), ('of', 499), ('in', 280), ('for', 201), ('me', 253), ('on', 77), ('you', 440), ('by', 108), ('a', 444), ('the', 832), ('I', 676)])\n",
      "\n",
      "('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/tempest.txt', [('at', 80), ('of', 435), ('in', 262), ('a', 355), ('you', 260), ('by', 106), ('the', 695), ('me', 165), ('I', 559), ('on', 85), ('for', 134)])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = f_wcL2_RDD.collect() \n",
    "for el in output[1:4]:\n",
    "    print(el)\n",
    "    print()\n",
    "# should show something like this:\n",
    "# ('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/king_lear.txt', \n",
    "# [('of', 495), ('in', 280), ('at', 66), ('me', 228), ('I', 705), ('for', 130), ('on', 104), ('you', 412), ('a', 364), ('the', 746), ('by', 84)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2e) Creating Hash Vectors\n",
    "\n",
    "If we want to use all the words in each text, we need to reduce the dimensionality of the vectors. For this we use the 'Hashing Trick' shown in the lecture. \n",
    "\n",
    "Start by writing a function that takes a (word,count) list, and transforms it into vector of fixed size. For that you need to take the have value of each word module the size (`hash(word) % size`) and add up all counts that map here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 0, 36, 0, 34]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hashWcList(lst,size):\n",
    "    lst2 = [0] * size;\n",
    "    for (w,c) in lst:\n",
    "        lst2[hash(w) % size] += c  # determine the position with hash(w)%size and add c there\n",
    "    return lst2\n",
    "        \n",
    "hashWcList([('this',23),('is',12),('a',34),('little',13),('test',24)],5) # for testing\n",
    "#output should look like this: [36, 0, 36, 0, 34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/hamlet.txt', [('in', 427), ('of', 685), ('at', 86), ('you', 544), ('I', 617), ('the', 1050), ('for', 202), ('on', 141), ('by', 124), ('a', 496), ('me', 236)])\n",
      "\n",
      "('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/merchant_of_venice.txt', [('at', 73), ('of', 499), ('in', 280), ('for', 201), ('me', 253), ('on', 77), ('you', 440), ('by', 108), ('a', 444), ('the', 832), ('I', 676)])\n",
      "\n",
      "('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bb-4c006897e4ec94-b51c55267e90/notebook/work/City-Data-Science/library/tempest.txt', [('at', 80), ('of', 435), ('in', 262), ('a', 355), ('you', 260), ('by', 106), ('the', 695), ('me', 165), ('I', 559), ('on', 85), ('for', 134)])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_hv_RDD = wordCountRDD.map(lambda f_wcl: (f_wcl[0],hashWcList(f_wcl[1],10)))\n",
    "\n",
    "for el in output[1:4]:\n",
    "    print(el)\n",
    "    print()\n",
    "# now we can display a 10-dimensional vector for every text file "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
