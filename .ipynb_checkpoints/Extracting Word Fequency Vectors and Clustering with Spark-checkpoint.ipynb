{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Word Fequency Vectors with Spark\n",
    "\n",
    "These tasks are for working in the lab session and during the week. We will use the same data as last week (19 files in './City-Data-Science/library/') and use some more RDD functions. We will apply two different approaches to create and use fixed size vectors.\n",
    "\n",
    "First update the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'City-Data-Science' already exists and is not an empty directory.\n",
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science\n",
      "From https://github.com/tweyde/City-Data-Science\n",
      " * branch            HEAD       -> FETCH_HEAD\n",
      "Already up-to-date.\n",
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tweyde/City-Data-Science.git\n",
    "%cd City-Data-Science/\n",
    "!git pull https://github.com/tweyde/City-Data-Science.git\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is code from last week that we run first, and then extend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prideandpredjudice', 'the'),\n",
       " ('prideandpredjudice', 'of'),\n",
       " ('prideandpredjudice', 'pride')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def stripFinalS( word ):\n",
    "    word = word.lower() # lower case\n",
    "    if len(word) >0 and word[-1] == 's': # check for final letter\n",
    "        return word[:-1]\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "def splitFileWords(filenameContent): # your splitting function\n",
    "    f,c = filenameContent # split the input tuple  \n",
    "    fwLst = [] # the new list for (filename,word) tuples\n",
    "    wLst = re.split('\\W+',c) # <<< now create a word list wLst\n",
    "    for w in wLst : # iterate through the list\n",
    "        fwLst.append((f,stripFinalS(w))) # and append (f,w) to the \n",
    "    return fwLst #return a list of (f,w) tuples \n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "dirPath = './City-Data-Science/library/' #  path\n",
    "ft_RDD = sc.wholeTextFiles(dirPath) #<<< add code to create an RDD with wholeTextFiles\n",
    "fnt_RDD = ft_RDD.map(lambda ft: (re.split('[/\\.]',ft[0])[-2],ft[1])) # just take filename, \n",
    "                                                # drop path and extension for readability\n",
    "fw_RDD1 = fnt_RDD.flatMap(splitFileWords)\n",
    "fw_RDD = fw_RDD1.filter(lambda fw: len(fw[1])>0 and fw[1] not in ['project','gutenberg', 'ebook'])  \n",
    "fw_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Exploring\n",
    "\n",
    "a) Count the number of documents.\n",
    "\n",
    "b) Determine the number distinct words in total (the vocabulary size) using RDD.distinct(). This involves removing the fs from the (f,w) pairs and geting getting the RDD size (with RDD.count()). \n",
    "\n",
    "c) Get the number of words (including repeated ones) per book. \n",
    "\n",
    "d) Determine the number of distinct words per book. This involves determining the disting (f,w) pairs, geting a list of words per file, and getting the list size.\n",
    "\n",
    "e) Count the average number of occurences per word per file (words/vocabulary). Use RDD.join() to get both numbers into one RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents:  18\n",
      "Total vocabulary size:  23368\n",
      "Total vocabulary size (v2):  23368\n",
      "Words per book:  [('othello', 30617), ('macbeth', 20810), ('lady_susan', 26109)]\n",
      "Words per book (v2):  defaultdict(<class 'int'>, {'macbeth': 20810, 'othello': 30617, 'prideandpredjudice': 125212, 'julius_cesar': 22959, 'lady_susan': 26109, 'midsummer': 19814, 'senseandsensibility': 123066, 'mansfield_park': 163509, 'merchant_of_venice': 24772, 'richard_III': 33785, 'king_lear': 30119, 'tempest': 27891, 'persuasion': 83680, 'northanger_abbey': 77582, 'hamlet': 34518, 'romeo_and_juliet': 29538, 'henry_V': 29900, 'emma': 163979})\n",
      "Vocabulary per book:  [('othello', 4077), ('macbeth', 3654), ('lady_susan', 3097)]\n",
      "Vocabulary per book (v2):  defaultdict(<class 'int'>, {'othello': 4077, 'mansfield_park': 7492, 'hamlet': 4447, 'prideandpredjudice': 6181, 'julius_cesar': 2834, 'lady_susan': 3097, 'king_lear': 3892, 'senseandsensibility': 6073, 'merchant_of_venice': 3686, 'richard_III': 3824, 'midsummer': 3448, 'tempest': 4727, 'macbeth': 3654, 'romeo_and_juliet': 3722, 'persuasion': 5268, 'northanger_abbey': 5510, 'henry_V': 4815, 'emma': 6667})\n",
      "[('othello', (30617, 4077)), ('macbeth', (20810, 3654)), ('lady_susan', (26109, 3097))]\n",
      "Average word occurences:  [('othello', 7.509688496443463), ('macbeth', 5.695128626163109), ('lady_susan', 8.430416532127866)]\n"
     ]
    }
   ],
   "source": [
    "# a) Library size\n",
    "print(\"Number of documents: \",ft_RDD.count())\n",
    "\n",
    "# b) Vocabulary size\n",
    "w_RDD = fw_RDD.map(lambda fw: fw[1])\n",
    "w_RDDu = w_RDD.distinct()\n",
    "print('Total vocabulary size: ',w_RDDu.count())\n",
    "# this can also be programmed in short as follows:\n",
    "print('Total vocabulary size (v2): ',fw_RDD.values().distinct().count())\n",
    "\n",
    "# c) words per book\n",
    "from operator import add\n",
    "f1_RDD = fw_RDD.map(lambda fw: (fw[0],1)) # swap and wrap (f,w) to (w,1)\n",
    "fc_RDD = f1_RDD.reduceByKey(add)\n",
    "print('Words per book: ',fc_RDD.take(3))\n",
    "# or alternatively (both versions should produce the same output):\n",
    "print('Words per book (v2): ',fw_RDD.countByKey())\n",
    "\n",
    "# d) Vocabulary per book\n",
    "fw_RDDu = fw_RDD.distinct() # get unique (f,w) pairs - i.e. evey word only once per file. I use postfix u to mark 'unique'\n",
    "f1_RDDu = fw_RDDu.map(lambda fw: (fw[0],1)) # swap and wrap (f,w) to (w,[f])\n",
    "fcu_RDD = f1_RDDu.reduceByKey(add)\n",
    "print('Vocabulary per book: ',fcu_RDD.take(3))\n",
    "# or, again, shorter:\n",
    "print('Vocabulary per book (v2): ',fw_RDD.distinct().countByKey())\n",
    "\n",
    "# e) Average occurences of words per book (i.e. words/vocab per book)\n",
    "f_wv_RDD = fc_RDD.join(fcu_RDD) # join the two RDDs to get (f,(w,v)) tuples\n",
    "print(f_wv_RDD.take(3)) \n",
    "f_awo_RDD = f_wv_RDD.map(lambda f_wv: (f_wv[0],f_wv[1][0]/f_wv[1][1])) # this is the tricky part. \n",
    "# Resolve nested tuples in the lambda to get (filename,words/vocab) tuples\n",
    "print('Average word occurences: ',f_awo_RDD.take(3))\n",
    "# should look like this [('henry_V', 6.237027812370278), ('king_lear', 7.815661103979461), ('lady_susan', 8.531763947113834)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Fixed vectors: Reduced vocabulary approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('prideandpredjudice', 'the'), 0), (('prideandpredjudice', 'a'), 0), (('prideandpredjudice', 'in'), 0)]\n",
      "[(('prideandpredjudice', 'the'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'by'), 1)]\n",
      "[(('prideandpredjudice', 'the'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'by'), 1)]\n",
      "[(('mansfield_park', 'the'), 6382), (('midsummer', 'in'), 265), (('prideandpredjudice', 'in'), 1937)]\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "stopwlst = ['the','a','in','of','on','at','for','by','i','you','me'] # stopword list\n",
    "fw_RDD2 = fw_RDD.filter(lambda x: x[1] in stopwlst) # filter, keeping only stopwords\n",
    "\n",
    "#fw_RDD.keys().collect()\n",
    "fsw_0_RDD = fw_RDD.keys().flatMap(lambda f: [((f,sw),0) for sw in stopwlst])\n",
    "print(fsw_0_RDD.take(3))\n",
    "\n",
    "fw_1_RDD = fw_RDD2.map(lambda x: (x,1))  #<<< change (f,w) to ((f,w),1)\n",
    "print(fw_1_RDD.take(3))\n",
    "\n",
    "fw_10_RDD = fw_1_RDD.union(fsw_0_RDD)\n",
    "print(fw_10_RDD.take(3))\n",
    "\n",
    "fw_c_RDD = fw_10_RDD.reduceByKey(add) #<<< count the words\n",
    "print(fw_c_RDD.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Creating sorted lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('othello', [('a', 668), ('at', 78), ('by', 131), ('for', 270), ('i', 1222), ('in', 349), ('me', 281), ('of', 542), ('on', 132), ('the', 820), ('you', 552)]), ('macbeth', [('a', 395), ('at', 64), ('by', 74), ('for', 142), ('i', 581), ('in', 227), ('me', 119), ('of', 427), ('on', 76), ('the', 765), ('you', 272)]), ('merchant_of_venice', [('a', 646), ('at', 75), ('by', 131), ('for', 254), ('i', 976), ('in', 319), ('me', 260), ('of', 535), ('on', 81), ('the', 938), ('you', 507)])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('macbeth',\n",
       "  [395.0, 64.0, 74.0, 142.0, 581.0, 227.0, 119.0, 427.0, 76.0, 765.0, 272.0]),\n",
       " ('lady_susan',\n",
       "  [611.0,\n",
       "   161.0,\n",
       "   152.0,\n",
       "   262.0,\n",
       "   1106.0,\n",
       "   402.0,\n",
       "   200.0,\n",
       "   787.0,\n",
       "   140.0,\n",
       "   784.0,\n",
       "   353.0]),\n",
       " ('othello',\n",
       "  [668.0,\n",
       "   78.0,\n",
       "   131.0,\n",
       "   270.0,\n",
       "   1222.0,\n",
       "   349.0,\n",
       "   281.0,\n",
       "   542.0,\n",
       "   132.0,\n",
       "   820.0,\n",
       "   552.0])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reGrpLst(fw_c): # we get a nested tuple\n",
    "    fw,c = fw_c     # split the outer tuple\n",
    "    f,w = fw        # split the inner tuple\n",
    "    return (f,[(w,c)]) # return (f,[(w,c)]) structure. Can be used verbatim, if your variable names match.\n",
    "\n",
    "fw_RDD\n",
    "f_wcL_RDD = fw_c_RDD.map(reGrpLst) \n",
    "f_wcL2_RDD = f_wcL_RDD.reduceByKey(add) #<<< create [(w,c), ... ,(w,c)] lists per file \n",
    "f_wcLsort_RDD = f_wcL2_RDD.map(lambda f_wcL: (f_wcL[0], sorted(f_wcL[1],key=lambda x: x[0]))) #<<< sort the lists\n",
    "print(f_wcLsort_RDD.take(3))\n",
    "f_wVec_RDD = f_wcLsort_RDD.map(lambda f_wc: (f_wc[0],[float(c) for (w,c) in f_wc[1]])) #<<< remove the words and convert the numbers to floats\n",
    "f_wVec_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Clustering\n",
    "\n",
    "Now we have feature vectors of fixed size, we can use KMeans as provided by Spark.\n",
    "\n",
    "The files in our library are by two authors. After clustering, check if the cluters reflect authorship:\n",
    "\n",
    "WILLIAM SHAKESPEARE: \n",
    "merchant_of_venice, \n",
    "richard_III, \n",
    "midsummer,\n",
    "tempest,\n",
    "romeoandjuliet,\n",
    "othello,\n",
    "henry_V,\n",
    "macbeth,\n",
    "king_lear,\n",
    "julius_cesar,\n",
    "hamlet\n",
    "\n",
    "JANE AUSTEN\n",
    "mansfield_park,\n",
    "emma,\n",
    "northanger_abbey,\n",
    "lady_susan,\n",
    "persuasion,\n",
    "prideandpredjudice,\n",
    "senseandsensibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('othello', 0)\n",
      "('macbeth', 0)\n",
      "('merchant_of_venice', 0)\n",
      "('persuasion', 1)\n",
      "('lady_susan', 0)\n",
      "('mansfield_park', 1)\n",
      "('julius_cesar', 0)\n",
      "('emma', 1)\n",
      "('tempest', 0)\n",
      "('richard_III', 0)\n",
      "('king_lear', 0)\n",
      "('northanger_abbey', 1)\n",
      "('senseandsensibility', 1)\n",
      "('prideandpredjudice', 1)\n",
      "('romeo_and_juliet', 0)\n",
      "('midsummer', 0)\n",
      "('henry_V', 0)\n",
      "('hamlet', 0)\n",
      "Within Set Sum of Squared Error = 15118.000047182637\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from math import sqrt\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans #, KMeansModel\n",
    "\n",
    "#print('f_wVec_RDD.take(2): ', f_wVec_RDD.take(1))\n",
    "wVec_RDD = f_wVec_RDD.map(lambda f_wcl: f_wcl[1]) # strip the filenames\n",
    "#print(wVec_RDD.collect())\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusterModel = KMeans.train(wVec_RDD, 2, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "# Assign the files to the clusters\n",
    "fc_RDD = f_wVec_RDD.map(lambda fv: (fv[0],clusterModel.predict(fv[1])))\n",
    "for s in fc_RDD.collect():\n",
    "    print(s)\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusterModel.centers[clusterModel.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = wVec_RDD.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Alternative approach: feature hashing\n",
    "\n",
    "Instead of the previous appraoch, we now use feature hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('othello', [3210, 3107, 2080, 3437, 3237, 3216, 2510, 4440, 1962, 3418]), ('macbeth', [1964, 2024, 1877, 2148, 2010, 2234, 1610, 3167, 1339, 2437]), ('lady_susan', [3448, 2639, 1874, 2737, 2847, 2894, 1994, 3851, 1158, 2667])]\n"
     ]
    }
   ],
   "source": [
    "def hashing_vectorizer(word_count_list, N):\n",
    "     v = [0] * N  # create fixed size vector of 0s\n",
    "     for word_count in word_count_list: \u000b",
    "         word,count = word_count \t# unpack tuple\n",
    "         h = hash(word) # get hash value\n",
    "         v[h % N] = v[h % N] + count # add count\n",
    "     return v \t# return hashed word vector\n",
    "\n",
    "from operator import add\n",
    "\n",
    "N = 10\n",
    "\n",
    "# we use fw_RDD from the beginning with all the words, not just stopwords\n",
    "fw_1_RDD = fw_RDD.map(lambda x: (x,1))  #<<< change (f,w) to ((f,w),1)\n",
    "fw_c_RDD = fw_1_RDD.reduceByKey(add) #as above\n",
    "f_wcL_RDD = fw_c_RDD.map(reGrpLst) #as above\n",
    "f_wcL2_RDD = f_wcL_RDD.reduceByKey(add) #<<< create [(w,c), ... ,(w,c)] lists per file \n",
    "f_wVec_RDD = f_wcL2_RDD.map(lambda f_wc: (f_wc[0],hashing_vectorizer(f_wc[1],N)))\n",
    "print(f_wVec_RDD.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('othello', 0)\n",
      "('macbeth', 0)\n",
      "('lady_susan', 0)\n",
      "('king_lear', 0)\n",
      "('senseandsensibility', 1)\n",
      "('merchant_of_venice', 0)\n",
      "('midsummer', 0)\n",
      "('prideandpredjudice', 1)\n",
      "('persuasion', 1)\n",
      "('northanger_abbey', 1)\n",
      "('romeo_and_juliet', 0)\n",
      "('henry_V', 0)\n",
      "('mansfield_park', 1)\n",
      "('tempest', 0)\n",
      "('richard_III', 0)\n",
      "('julius_cesar', 0)\n",
      "('hamlet', 0)\n",
      "('emma', 1)\n",
      "Within Set Sum of Squared Error = 76523.43917015033\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans #, KMeansModel\n",
    "\n",
    "#print('f_wVec_RDD.take(2): ', f_wVec_RDD.take(1))\n",
    "wVec_RDD = f_wVec_RDD.map(lambda f_wcl: f_wcl[1]) # strip the filenames\n",
    "#print(wVec_RDD.collect())\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusterModel = KMeans.train(wVec_RDD, 2, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "# Assign the files to the clusters\n",
    "fc_RDD = f_wVec_RDD.map(lambda fv: (fv[0],clusterModel.predict(fv[1])))\n",
    "for s in fc_RDD.collect():\n",
    "    print(s)\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusterModel.centers[clusterModel.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = wVec_RDD.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6) Neutralising document length: Normalised vectors\n",
    "\n",
    "'Lady Susan' ends up reliably in the wrong cluster. A possible explanation is that it is shorter than the other Austen works. Try normalising the word counts, i.e. by dividing by their sum. That takes away the effect of length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalised vectors:  [[0.09437770302739068, 0.0972609322441134, 0.09019702066314272, 0.10321960595867372, 0.09658817876021143, 0.10735223450264296, 0.07736665064872657, 0.1521864488226814, 0.06434406535319558, 0.11710716001922153], [0.10484371427638241, 0.10147957017343306, 0.0679361139236372, 0.11225789594016396, 0.10572557729366039, 0.10503968383577751, 0.08198059901361988, 0.1450174739523794, 0.06408204592220008, 0.11163732566874612], [0.1320617411620514, 0.1010762572293079, 0.07177601593320311, 0.10482975219273048, 0.10904285878432725, 0.1108430043280095, 0.07637213221494504, 0.14749703167490139, 0.044352522118809606, 0.10214868436171436]]\n",
      "('othello', 0)\n",
      "('macbeth', 0)\n",
      "('lady_susan', 0)\n",
      "('king_lear', 0)\n",
      "('senseandsensibility', 0)\n",
      "('merchant_of_venice', 0)\n",
      "('midsummer', 0)\n",
      "('prideandpredjudice', 0)\n",
      "('persuasion', 0)\n",
      "('northanger_abbey', 0)\n",
      "('romeo_and_juliet', 0)\n",
      "('henry_V', 0)\n",
      "('mansfield_park', 0)\n",
      "('tempest', 0)\n",
      "('richard_III', 0)\n",
      "('julius_cesar', 0)\n",
      "('hamlet', 0)\n",
      "('emma', 0)\n"
     ]
    }
   ],
   "source": [
    "nwVec_RDD = wVec_RDD.map(lambda v: ([c/sum(v) for c in v])) \n",
    "# provide a list compresention that normalises the values by the \n",
    "print(\"Normalised vectors: \",nwVec_RDD.take(3))\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusterModel = KMeans.train(nwVec_RDD, 2, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "# Assign the files to the clusters\n",
    "fc_RDD = f_wVec_RDD.map(lambda fv: (fv[0],clusterModel.predict(fv[1])))\n",
    "for s in fc_RDD.collect():\n",
    "    print(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Building an index\n",
    "\n",
    "Starting from the fw_RDD we now start building the index and calculating the IDF values. Since we have the TF values alread, we only need to keep the unique filenames per word using [RDD.distinct()](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.distinct).  \n",
    "Then we create a list of filenames. The length of the list is the document frequency DF per word.\n",
    "From the DF value we can calculate the IDF value as log(18/DF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('iwi', ['richard_III', 'merchant_of_venice']), ('matrimonial', ['emma', 'mansfield_park', 'lady_susan', 'prideandpredjudice']), ('circle', ['tempest', 'mansfield_park', 'emma', 'northanger_abbey', 'king_lear', 'senseandsensibility', 'persuasion', 'romeo_and_juliet', 'henry_V', 'prideandpredjudice'])]\n",
      "DF:  [('iwi', 2), ('calme', 1), ('circle', 10)]\n",
      "IDF:  [('iwi', 2.1972245773362196), ('matrimonial', 1.5040773967762742), ('circle', 0.5877866649021191)]\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "from math import log\n",
    "\n",
    "fwu_RDD = fw_RDD.distinct() # get unique file/word pairs\n",
    "wfl_RDD = fwu_RDD.map(lambda fw: (fw[1],[fw[0]])) # create (w,[f]) tuples \n",
    "wfL_RDD = wfl_RDD.reduceByKey(add) # concatenate the lists with 'add'\n",
    "print(wfL_RDD.take(3))\n",
    "\n",
    "wdf_RDD = wfL_RDD.map(lambda wfl: (wfl[0],len(wfl[1]))) # get the DF replacing the file list with its lenght\n",
    "print(\"DF: \",wdf_RDD.take(3))\n",
    "widf_RDD = wdf_RDD.map(lambda wdf: (wdf[0],log(18/wdf[1]))) # get he IDF by replacing DF with log(19/DF)\n",
    "print(\"IDF: \",widf_RDD.take(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
