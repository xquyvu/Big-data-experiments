{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten Digit Classfication using Convolutional Neural Network\n",
    "\n",
    "## TASK 1\n",
    "\n",
    "In this lab we are going to look at an example of classifying handwritten digits using BigDL. We will introduce the convolution operation and gain familiarity with the different parameters in CNNs. \n",
    "\n",
    "\n",
    "Goal: \n",
    "\n",
    "Our goal is to train a classifier that will identify the digits in the MNIST dataset.\n",
    "\n",
    "Approach:\n",
    "\n",
    "There are 5 stages: Data reading, Data preprocessing, Creating a model, Learning the model parameters and Evaluating (a.k.a. testing/prediction) the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!(export sv=2.1 bv=0.3.0 ; cd ~/data/libs/ && wget https://repo1.maven.org/maven2/com/intel/analytics/bigdl/bigdl-SPARK_${sv}/${bv}/bigdl-SPARK_${sv}-${bv}-jar-with-dependencies.jar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bigdl==0.3.0 | cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to import the packages needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%pylab inline\n",
    "\n",
    "import pandas\n",
    "import datetime as dt\n",
    "\n",
    "from bigdl.nn.layer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.dataset import mnist\n",
    "#from utils import get_mnist\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark import SparkContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Creation\n",
    "\n",
    "CNN is a feedforward network made up of bunch of layers in such a way that the output of one layer becomes the input to the next layer (similar to MLP). In MLP, all possible pairs of input pixels are connected to the output nodes with each pair having a weight, thus leading to a combinatorial explosion of parameters to be learnt and also increasing the possibility of overfitting (details). Convolution layers take advantage of the spatial arrangement of the pixels and learn multiple filters that significantly reduce the amount of parameters in the network (details). The size of the filter is a parameter of the convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LeNet-5 model\n",
    "#The 1st convolutional layer takes Input = 28x28x1 ; Output = 24x24x6\n",
    "#Math = 6 unique (28-5+1),(28-5+1) ==>24x24x6\n",
    "#The 1st pooling takes Input = 24x24x6; Output = 12x12x6\n",
    "#Math = For a stride of 2,2 (24,24) = output ==>(12x12)\n",
    "#The 2nd convolution layer takes Input = 12x12x6 ; Output = 8x8x12\n",
    "#Math = 12 unique (12-5+1),(12-5+1) ==> 8x8x12\n",
    "#The 2nd pooling takes Input = 8x8x12; Output = 4x4x12\n",
    "#Math = For a stride (2,2) (8,8) = outputs ==>(4x4)\n",
    "\n",
    "#Next step is to create a fully connected layer with input 4x4x12 and Output = 100\n",
    "#Next layer will also be a fully connecte layer with Input 100 and output = class_num\n",
    "#class_num is the number of output layer = 10 (MNIST 0-9 numbers)\n",
    "#the softmax layer will ensure we get a probability distribution\n",
    "\n",
    "\n",
    "def build_model(class_num):\n",
    "    model = Sequential()\n",
    "    model.add(Reshape([1, 28, 28]))  #The image shape is 28x28x1 \n",
    "    model.add(SpatialConvolution(1, 6, 5, 5).set_name('conv1'))  #this is the first convolutional layer\n",
    "    model.add(Tanh())   #add an activation function\n",
    "    model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool1'))     \n",
    "    model.add(Tanh())   #add an activation function \n",
    "    model.add(SpatialConvolution(6, 12, 5, 5).set_name('conv2'))  #this is the second convolutional layer\n",
    "    model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool2'))\n",
    "    model.add(Reshape([12 * 4 * 4]))\n",
    "    model.add(Linear(12 * 4 * 4, 100).set_name('fc1')) #1st Fully connected layer\n",
    "    model.add(Tanh())\n",
    "    model.add(Linear(100, class_num).set_name('score'))\n",
    "    model.add(LogSoftMax())\n",
    "    return model\n",
    "\n",
    "#This will build a model with 10 output layers\n",
    "\n",
    "lenet_model = build_model(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now start with creating a new spark context and get the BigDL engine started\n",
    "\n",
    "sc.stop()            #This is to ensure we first stop any instance already running\n",
    "confCore=create_spark_conf()\n",
    "confCore.set(\"spark.executor.cores\", 1)\n",
    "confCore.set(\"spark.cores.max\", 1)\n",
    "sc = SparkContext(appName=\"Mnist\", conf=confCore)\n",
    "init_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and store MNIST into RDD of Sample\n",
    "def get_mnist(sc, data_type=\"train\", location=\"/tmp/mnist\"):\n",
    "    \"\"\"\n",
    "    Get and normalize the mnist data. We would download it automatically\n",
    "    if the data doesn't present at the specific location.\n",
    "    :param sc: SparkContext\n",
    "    :param data_type: training data or testing data\n",
    "    :param location: Location storing the mnist\n",
    "    :return: A RDD of (features: Ndarray, label: Ndarray)\n",
    "    \"\"\"\n",
    "    (images, labels) = mnist.read_data_sets(location, data_type)   #Seperating the data into images and labels respectively\n",
    "    images = sc.parallelize(images)\n",
    "    labels = sc.parallelize(labels + 1) # Target start from 1 in BigDL\n",
    "    record = images.zip(labels)\n",
    "    return record\n",
    "\n",
    "def get_end_trigger():\n",
    "        return MaxEpoch(10)\n",
    "\n",
    "train_data = get_mnist(sc, \"train\", \"\")\\\n",
    "    .map(lambda rec_tuple: (normalizer(rec_tuple[0], mnist.TRAIN_MEAN, mnist.TRAIN_STD),\n",
    "                       rec_tuple[1]))\\\n",
    "    .map(lambda t: Sample.from_ndarray(t[0], t[1]))\n",
    "test_data = get_mnist(sc, \"test\", \"\")\\\n",
    "    .map(lambda rec_tuple: (normalizer(rec_tuple[0], mnist.TEST_MEAN, mnist.TEST_STD),\n",
    "                       rec_tuple[1]))\\\n",
    "    .map(lambda t: Sample.from_ndarray(t[0], t[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING AN OPTIMIZER \n",
    "\n",
    "An optimizer is in general to minimize any function with respect to a set of parameters. In case of training a neural network, an optimizer tries to minimize the loss of the neural net with respect to its weights/biases, over the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Optimizer\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    model=build_model(10),\n",
    "    training_rdd=train_data,\n",
    "    criterion=ClassNLLCriterion(),\n",
    "    optim_method=SGD(learningrate=0.4, learningrate_decay=0.0002),\n",
    "    end_trigger=MaxEpoch(20),\n",
    "    batch_size=2048)\n",
    "\n",
    "#Set the validation logic\n",
    "#Function .setValidation is to set a validate evaluation in the optimizer.\n",
    "#trigger: how often to evaluation validation set.\n",
    "#sampleRDD: validate data set in type of RDD[Sample].\n",
    "#vMethods: a set of ValidationMethod.\n",
    "#batchSize: size of mini batch. \n",
    "\n",
    "optimizer.set_validation(\n",
    "    batch_size=2048,\n",
    "    val_rdd=test_data,\n",
    "    trigger=EveryEpoch(),\n",
    "    val_method=[Top1Accuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here TrainSummary and ValidationSummary are in built functions to save logs.\n",
    "# We can save the logs to a tmp file on DSX \n",
    "#the idea behind creating an app_name is to save different models\n",
    "app_name='lenet-'+dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_summary = TrainSummary(log_dir='/tmp',\n",
    "                                     app_name=app_name)\n",
    "train_summary.set_summary_trigger(\"Parameters\", SeveralIteration(50))\n",
    "val_summary = ValidationSummary(log_dir='/tmp',\n",
    "                                        app_name=app_name)\n",
    "optimizer.set_train_summary(train_summary)\n",
    "optimizer.set_val_summary(val_summary)\n",
    "print(\"saving logs to \",app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## START TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training may take about ~7 minutes\n",
    "#Function optimize will start the training.\n",
    "# Boot training process\n",
    "trained_model = optimizer.optimize()\n",
    "print(\"Optimization Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_predict_label(l):\n",
    "    return np.array(l).argmax()\n",
    "def map_groundtruth_label(l):\n",
    "    return l[0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label-1 to restore the original label.\n",
    "print(\"Ground Truth labels:\") \n",
    "print(', '.join([str(map_groundtruth_label(s.label.to_ndarray())) for s in train_data.take(8)]))\n",
    "imshow(np.column_stack([np.array(s.features[0].to_ndarray()).reshape(28,28) for s in train_data.take(8)]),cmap='gray'); plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERFORM  TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Use the trained model to test some unseen values\n",
    "predictions = trained_model.predict(test_data)\n",
    "imshow(np.column_stack([np.array(s.features[0].to_ndarray()).reshape(28,28) for s in test_data.take(8)]),cmap='gray'); plt.axis('off')\n",
    "print('Ground Truth labels:')\n",
    "print(', '.join(str(map_groundtruth_label(s.label.to_ndarray())) for s in test_data.take(8)))\n",
    "print('Predicted labels:')\n",
    "print(','.join([str(map_predict_label(s)) for s in predictions.take(8)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARAMETER INSPECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the parameters of the trained model\n",
    "#The parameters are exposed as a dict, and can be retrieved using model.parameters().\n",
    "\n",
    "params = trained_model.parameters()\n",
    "\n",
    "#batch num, output_dim, input_dim, spacial_dim\n",
    "for layer_name, param in params.items():\n",
    "    print (layer_name,param['weight'].shape,param['bias'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALISE WEIGHTS OF CONVOLUTIONAL LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis_square is borrowed from caffe example\n",
    "def vis_square(data):\n",
    "    \"\"\"Take an array of shape (n, height, width) or (n, height, width, 3)\n",
    "       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\"\"\"\n",
    "    \n",
    "    # normalize data for display\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "    # force the number of filters to be square\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = (((0, n ** 2 - data.shape[0]),\n",
    "               (0, 1), (0, 1))                 # add some space between filters\n",
    "               + ((0, 0),) * (data.ndim - 3))  # don't pad the last dimension (if there is one)\n",
    "    data = np.pad(data, padding, mode='constant', constant_values=1)  # pad with ones (white)\n",
    "    \n",
    "    # tile the filters into an image\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "  \n",
    "    plt.imshow(data,cmap='gray'); plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_conv1 = params['conv1']['weight'] # we are extracting weights for the 1st convolutional layer for visualizing  \n",
    "\n",
    "filters_conv1[0,0,0]   # this will return the 1st element in the weight array\n",
    "\n",
    "vis_square(np.squeeze(filters_conv1, axis=(0,)).reshape(1*6,5,5))  # this concatenates the first two dimensions for ease of visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the parameters are a list of [weights, biases]\n",
    "filters_conv2 = params['conv2']['weight']    # we are extracting weights for the 2nd convolutional layer for visualizing\n",
    "\n",
    "vis_square(np.squeeze(filters_conv2, axis=(0,)).reshape(12*6,5,5))   # this concatenates the first two dimensions for ease of visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSS VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.array(train_summary.read_scalar(\"Loss\"))\n",
    "top1 = np.array(val_summary.read_scalar(\"Top1Accuracy\"))\n",
    "\n",
    "plt.figure(figsize = (12,12))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(loss[:,0],loss[:,1],label='loss')\n",
    "plt.xlim(0,loss.shape[0]+10)\n",
    "plt.grid(True)\n",
    "plt.title(\"loss\")\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(top1[:,0],top1[:,1],label='top1')\n",
    "plt.xlim(0,loss.shape[0]+10)\n",
    "plt.title(\"top1 accuracy\")\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
