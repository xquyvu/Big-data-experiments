{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification, Semi-structured text processing on Spark\n",
    "\n",
    "We are going to use a simple text classification problem as an example to build an ML pipeline in Spark MLlib, inspect it if it doesn't work as expected, and tune hyperparameters. We will also do some web scraping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1 - PROCESSING\n",
    "## 1) Dataset: Newsgroups\n",
    "\n",
    "This dataset contains messages from 20 different newsgroups on different topics with ~1000 messages each. More information and the data can be found here here:\n",
    "\n",
    "[http://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/](http://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/)\n",
    "\n",
    "With the larger dataset we should get more meaningful time measurements. We'll try several runs and try executing things in different oder (here we are only training a LR classifier).\n",
    "\n",
    "To create a meaningful dataset for classification, we need at least 2 topics and then use `RDD.randomSplit()`. This time, we will use alt.atheism and comp.graphics.\n",
    "\n",
    "**Download and unpack the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/datasets\n",
      "--2018-03-14 18:12:01--  http://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/20_newsgroups.tar.gz\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 17332201 (17M) [application/x-gzip]\n",
      "Saving to: ‘20_newsgroups.tar.gz.2’\n",
      "\n",
      "100%[======================================>] 17,332,201  1.22MB/s   in 12s    \n",
      "\n",
      "2018-03-14 18:12:13 (1.42 MB/s) - ‘20_newsgroups.tar.gz.2’ saved [17332201/17332201]\n",
      "\n",
      "20_newsgroups\t\t20_newsgroups.tar.gz.2\tlingspam_public\n",
      "20_newsgroups.tar.gz\tfoodhygiene\t\tlingspam_public02.tar.gz\n",
      "20_newsgroups.tar.gz.1\tfoodhygiene.zip\n"
     ]
    }
   ],
   "source": [
    "%cd ~/notebook/work/City-Data-Science/datasets/\n",
    "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/20_newsgroups.tar.gz\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Unzipping finished.\n"
     ]
    }
   ],
   "source": [
    "!tar -xf 20_newsgroups.tar.gz\n",
    "# '!' calls a program on the machine (the DSX service runs on virtual Linux machines).\n",
    "print(\">>> Unzipping finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"datasets/20_newsgroups/**\" >> ~/notebook/work/City-Data-Science/.git/info/exclude\n",
    "# add the newly created directory to the list of excluded dirs to prevent accidental uploading to github\n",
    "# do this only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './20_newsgroups/'\n",
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work\n",
      "\u001b[0m\u001b[01;34mCity-Data-Science\u001b[0m/  iris (2).csv  iris.csv                 \u001b[34;42mtest-Big-Data\u001b[0m/\n",
      "\u001b[34;42mcity_ds\u001b[0m/            iris (3).csv  Lab1-Solutions-v3.ipynb  \u001b[34;42mtestdsx\u001b[0m/\n",
      "iris (1).csv        iris (4).csv  \u001b[34;42mmy_model\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "# go into the new directory\n",
    "%cd ./20_newsgroups/ \n",
    "%ls # and show its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.9</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771\n",
      "p:  /gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/datasets/20_newsgroups\n",
      "Number of documents read is: 2000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.param import *\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "import os.path\n",
    "%cd ~\n",
    "p = os.path.abspath('./notebook/work/City-Data-Science/datasets/20_newsgroups/')\n",
    "print(\"p: \", p) # we need the absolute path, as the executors that will read the from directories, will not run in the same environment, so that %cd calls don't help\n",
    "\n",
    "#here we are setting the path to select 2 topics\n",
    "dirPath1 = p + '/alt.atheism'\n",
    "dirPath2 = p + '/comp.graphics'\n",
    "\n",
    "# Use wholeTextFiles to read both the files\n",
    "alt_rdd = sc.wholeTextFiles(dirPath1)\n",
    "comp_rdd = sc.wholeTextFiles(dirPath2)\n",
    "\n",
    "alt_rdd.take(1)\n",
    "comp_rdd.take(1)\n",
    "\n",
    "#Create a union of the 2 RDD's so we hava a full set\n",
    "newsGroup_RDD = alt_rdd.union(comp_rdd)\n",
    "\n",
    "#print the total number of documents here:\n",
    "print ('Number of documents read is:',newsGroup_RDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('alt.atheism', 'Path: cantaloupe.srv.cs.cmu.edu!magnesium.club.cc.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!zaphod.mps.ohio-state.edu!howland.reston.ans.net!bogus.sura.net!news-feed-1.peachnet.edu!umn.edu!uum1!mac.cc.macalstr.edu!acooper\\nFrom: acooper@mac.cc.macalstr.edu (Turin Turambar, ME Department of Utter Misery)\\nNewsgroups: alt.atheism\\nSubject: Re: free moral agency\\nMessage-ID: <1993Apr20.185237.4924@mac.cc.macalstr.edu>\\nDate: 20 Apr 93 18:52:37 -0600\\nReferences: <1quuaa$6s@eagle.lerc.nasa.gov> <735295730.25282@minster.york.ac.uk>\\nDistribution: world\\nOrganization: Macalester College\\nLines: 19\\n\\nIn article <735295730.25282@minster.york.ac.uk>, cjhs@minster.york.ac.uk writes:\\n> : Are you saying that their was a physical Adam and Eve, and that all\\n> : humans are direct decendents of only these two human beings.?  Then who\\n> : were Cain and Able\\'s wives?  Couldn\\'t be their sisters, because A&E\\n> : didn\\'t have daughters.  Were they non-humans?\\n> \\n> Genesis 5:4\\n> \\n> and the days of Adam after he begat Seth were eight hundred years, and\\n> he begat sons and daughters:\\n> \\n> Felicitations -- Chris Ho-Stuart\\n\\n\\nIt is still incestuous.... :)\\n\\n\\n\\n--Adam \"What happened to my sig?\"  Cooper\\n'), ('alt.atheism', \"Newsgroups: alt.atheism\\nPath: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!apple!mumbo.apple.com!gallant.apple.com!sandvik-kent.apple.com!user\\nFrom: sandvik@newton.apple.com (Kent Sandvik)\\nSubject: Re: some thoughts.\\nSender: news@gallant.apple.com\\nMessage-ID: <sandvik-150493145812@sandvik-kent.apple.com>\\nDate: Thu, 15 Apr 1993 22:00:41 GMT\\nReferences: <bissda.4.734849678@saturn.wwc.edu>\\nOrganization: Cookamunga Tourist Bureau\\nFollowup-To: alt.atheism\\nLines: 24\\n\\nIn article <bissda.4.734849678@saturn.wwc.edu>, bissda@saturn.wwc.edu (DAN\\nLAWRENCE BISSELL) wrote:\\n> \\n> \\tFirst I want to start right out and say that I'm a Christian.  It \\n> makes sense to be one.  Have any of you read Tony Campollo's book- liar, \\n> lunatic, or the real thing?  (I might be a little off on the title, but he \\n> writes the book.  Anyway he was part of an effort to destroy Christianity, \\n> in the process he became a Christian himself.\\n\\nSeems he didn't understand anything about realities, liar, lunatic\\nor the real thing is a very narrow view of the possibilities of Jesus\\nmessage.\\n\\nSigh, it seems religion makes your mind/brain filter out anything\\nthat does not fit into your personal scheme. \\n\\nSo anyone that thinks the possibilities with Jesus is bound to the\\nclassical Lewis notion of 'liar, lunatic or saint' is indeed bound\\nto become a Christian.\\n\\nCheers,\\nKent\\n---\\nsandvik@newton.apple.com. ALink: KSAND -- Private activities on the net.\\n\")]\n"
     ]
    }
   ],
   "source": [
    "#Split the filename and content\n",
    "\n",
    "import re \n",
    "    \n",
    "def splitFileWords(filenameContent):\n",
    "    f,c = filenameContent # split the input tuple  \n",
    "    fwLst = [] # the new list for (filename,word) tuples\n",
    "    wLst = re.split('\\W+',c) # <<< now create a word list wLst\n",
    "    for w in wLst : # iterate through the list\n",
    "        fwLst.append((f,w)) # and append (f,w) to the \n",
    "    return fwLst #return a list of (f,w) tuples\n",
    "\n",
    "\n",
    "# Remove the file name and path before the last directory name (i.e. the newsgroup name) \n",
    "fnt_RDD = newsGroup_RDD.map(lambda ft: (re.split('[/]',ft[0])[-2],ft[1]))\n",
    "print(fnt_RDD.take(2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Preprocessing: Remove the headers from the files\n",
    "\n",
    "At closer inspection, we can see that the messages have headers, and one of them starts with 'Newsgroups:' and actually lists the topic. This is clearly an unreasonable shortcut for the classifier, as we are interested in predicting topics from the text.  \n",
    "\n",
    "Thus, the dataset needs preprocessing to remove these headers. We can use a regular expression to remove the header. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alt.atheism',\n",
       "  ' 19\\n\\nIn article <735295730.25282@minster.york.ac.uk>, cjhs@minster.york.ac.uk writes:\\n> : Are you saying that their was a physical Adam and Eve, and that all\\n> : humans are direct decendents of only these two human beings.?  Then who\\n> : were Cain and Able\\'s wives?  Couldn\\'t be their sisters, because A&E\\n> : didn\\'t have daughters.  Were they non-humans?\\n> \\n> Genesis 5:4\\n> \\n> and the days of Adam after he begat Seth were eight hundred years, and\\n> he begat sons and daughters:\\n> \\n> Felicitations -- Chris Ho-Stuart\\n\\n\\nIt is still incestuous.... :)\\n\\n\\n\\n--Adam \"What happened to my sig?\"  Cooper\\n'),\n",
       " ('alt.atheism',\n",
       "  \" 24\\n\\nIn article <bissda.4.734849678@saturn.wwc.edu>, bissda@saturn.wwc.edu (DAN\\nLAWRENCE BISSELL) wrote:\\n> \\n> \\tFirst I want to start right out and say that I'm a Christian.  It \\n> makes sense to be one.  Have any of you read Tony Campollo's book- liar, \\n> lunatic, or the real thing?  (I might be a little off on the title, but he \\n> writes the book.  Anyway he was part of an effort to destroy Christianity, \\n> in the process he became a Christian himself.\\n\\nSeems he didn't understand anything about realities, liar, lunatic\\nor the real thing is a very narrow view of the possibilities of Jesus\\nmessage.\\n\\nSigh, it seems religion makes your mind/brain filter out anything\\nthat does not fit into your personal scheme. \\n\\nSo anyone that thinks the possibilities with Jesus is bound to the\\nclassical Lewis notion of 'liar, lunatic or saint' is indeed bound\\nto become a Christian.\\n\\nCheers,\\nKent\\n---\\nsandvik@newton.apple.com. ALink: KSAND -- Private activities on the net.\\n\"),\n",
       " ('alt.atheism',\n",
       "  ' 17\\n\\n>DATE:   20 Apr 93 05:23:15 GMT\\n>FROM:   Bake Timmons <timmbake@mcl.ucsb.edu>\\n>\\n>>Remember, Koresh \"dried\" for your sins.\\n>>\\n>>And pass that beef jerky.  Umm Umm.\\n>\\n>Though I wasn\\'t there, at least I can rely on you now to keep me posted on what\\n>what he\\'s doing.\\n>\\n\\nWhat\\nA \\nCook\\nOff !\\n\\n\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# new function to remove the headers using regular expressions\n",
    "def removeHeader(ft): \n",
    "    fn,text = ft # unpack the filename and text content\n",
    "    # use a regular expression to match the text\n",
    "    matchObj = re.match(r'.+^Lines:(.*)', text,re.DOTALL|re.MULTILINE) \n",
    "    if(matchObj): # only if the pattern has matched \n",
    "        text = matchObj.group(1) # can we replace the text, \n",
    "        #otherwise we keep the old for now (what could be a better solution?)\n",
    "    return (fn,text)\n",
    "\n",
    "fnt_RDD2 = fnt_RDD.map(removeHeader)\n",
    "fnt_RDD2.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) DataFrames\n",
    "\n",
    "In this section we will introduce Dataframes. To read more on DataFrames look here:\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "In Spark we can create Datframes from RDDs and that is what we will implement in the next section. \n",
    "A dataframe represents a table structure. We defined a schema that contains the names and types of the coumns in the table.\n",
    "\n",
    "From the official documentation:\n",
    "\n",
    "A Datframe can be created programatically in 3 steps:\n",
    "\n",
    "- Create an RDD of Rows from the original RDD;\n",
    "- Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.\n",
    "- Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.\n",
    "\n",
    "More on pyspark API can be found here:\n",
    "\n",
    "http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "barChart",
      "keyFields": "topic"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">.pd_warning{display:none;}</style><div class=\"pd_warning\"><em>Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter</em></div>\n",
       "        <div class=\"pd_save is-viewer-good\" style=\"padding-right:10px;text-align: center;line-height:initial !important;font-size: xx-large;font-weight: 500;color: coral;\">\n",
       "            \n",
       "        </div>\n",
       "    <div id=\"chartFigurebeed0992\" class=\"pd_save is-viewer-good\" style=\"overflow-x:auto\">\n",
       "            \n",
       "                    <script class=\"pd_save\">\n",
       "                    function setChartScript() {\n",
       "                        if (!window.Bokeh) {\n",
       "                            setTimeout(setChartScript, 250)\n",
       "                        } else {\n",
       "                            var d = document.getElementById(\"pd-bkchartdiv-beed0992\")\n",
       "                            if (d){\n",
       "                                var el = document.createElement('div')\n",
       "                                el.innerHTML = `\n",
       "<script type=\"text/javascript\">\n",
       "  (function() {\n",
       "    var fn = function() {\n",
       "      Bokeh.safely(function() {\n",
       "        (function(root) {\n",
       "          function embed_document(root) {\n",
       "            \n",
       "          var docs_json = '{\"8b8f9dae-f2f0-4721-9037-f3a16f3ee289\":{\"roots\":{\"references\":[{\"attributes\":{\"source\":{\"id\":\"9d831602-ee07-4fea-b997-13e71889109b\",\"type\":\"ColumnDataSource\"}},\"id\":\"87e92aed-3760-4afb-9c54-4197e9ba9daa\",\"type\":\"CDSView\"},{\"attributes\":{\"axis_label\":\"topic\",\"formatter\":{\"id\":\"08868477-0e87-493d-90b0-a4bcac196dbc\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":1,\"major_label_text_color\":{\"value\":null},\"major_label_text_font_size\":{\"value\":\"0px\"},\"major_tick_line_color\":{\"value\":null},\"minor_tick_line_color\":{\"value\":null},\"plot\":{\"id\":\"b96d4bef-177e-4662-a4d6-018226782c9b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"9cba2bbb-2b83-40b4-a364-9392411ed894\",\"type\":\"CategoricalTicker\"}},\"id\":\"3b719cf9-ea99-42ed-b30f-04ad25208b01\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"2c38e1c4-34ba-4ae4-89c1-2d31b1a9f872\",\"type\":\"HelpTool\"},{\"attributes\":{\"toolbar\":{\"id\":\"152d1e46-4a54-41be-96d8-cb1c4fef19d4\",\"type\":\"ProxyToolbar\"},\"toolbar_location\":\"above\"},\"id\":\"30ae4915-0921-48e1-8e08-70a0828ae80a\",\"type\":\"ToolbarBox\"},{\"attributes\":{\"axis_label\":\"pd_count\",\"formatter\":{\"id\":\"215e9e3f-5dba-49e4-af70-2368cd373e80\",\"type\":\"BasicTickFormatter\"},\"minor_tick_line_color\":{\"value\":null},\"plot\":{\"id\":\"b96d4bef-177e-4662-a4d6-018226782c9b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"ab744692-ed5a-4b63-b53f-53f4e24521cf\",\"type\":\"BasicTicker\"}},\"id\":\"8a5c5364-8aa5-4d2b-9f26-ed4857bef0c6\",\"type\":\"LinearAxis\"},{\"attributes\":{\"items\":[{\"id\":\"7baff1fc-0ac8-4f21-8b34-944d86436ffa\",\"type\":\"LegendItem\"}],\"location\":\"top_left\",\"plot\":{\"id\":\"b96d4bef-177e-4662-a4d6-018226782c9b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"908617d8-7332-47d6-ad06-3a089a501302\",\"type\":\"Legend\"},{\"attributes\":{\"fill_color\":{\"field\":\"x\",\"transform\":{\"id\":\"c95c36a1-2a66-4b54-85f8-1756606044d0\",\"type\":\"CategoricalColorMapper\"}},\"line_color\":{\"field\":\"x\",\"transform\":{\"id\":\"c95c36a1-2a66-4b54-85f8-1756606044d0\",\"type\":\"CategoricalColorMapper\"}},\"top\":{\"field\":\"counts\"},\"width\":{\"value\":0.925},\"x\":{\"field\":\"x\"}},\"id\":\"9a3e4611-ecaa-460d-b8f9-c814aedb9334\",\"type\":\"VBar\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"25f0ea90-2da2-4c9a-a70e-8c5749df5010\",\"type\":\"PanTool\"},{\"id\":\"217574b9-2208-4391-b631-27d39be87bde\",\"type\":\"WheelZoomTool\"},{\"id\":\"1731464f-3304-4ee2-9fc4-09dab87fd367\",\"type\":\"BoxZoomTool\"},{\"id\":\"494f14da-0495-47ea-9733-6012d9679584\",\"type\":\"SaveTool\"},{\"id\":\"ce42fc11-29f6-486e-869e-3b72d8ee2a2c\",\"type\":\"ResetTool\"},{\"id\":\"2c38e1c4-34ba-4ae4-89c1-2d31b1a9f872\",\"type\":\"HelpTool\"}]},\"id\":\"1a712119-f61f-438c-9564-754b5c6d7889\",\"type\":\"Toolbar\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"counts\"},\"width\":{\"value\":0.925},\"x\":{\"field\":\"x\"}},\"id\":\"36cdd6ae-3c18-4cf5-81df-79fd12269e67\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"08868477-0e87-493d-90b0-a4bcac196dbc\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"label\":{\"field\":\"l\"},\"renderers\":[{\"id\":\"7bfc5451-a4fc-4e80-b062-e6050d039949\",\"type\":\"GlyphRenderer\"}]},\"id\":\"7baff1fc-0ac8-4f21-8b34-944d86436ffa\",\"type\":\"LegendItem\"},{\"attributes\":{\"overlay\":{\"id\":\"ad5dbfd9-34e7-4f0f-8efd-24dde6aeede2\",\"type\":\"BoxAnnotation\"}},\"id\":\"1731464f-3304-4ee2-9fc4-09dab87fd367\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"tools\":[{\"id\":\"25f0ea90-2da2-4c9a-a70e-8c5749df5010\",\"type\":\"PanTool\"},{\"id\":\"217574b9-2208-4391-b631-27d39be87bde\",\"type\":\"WheelZoomTool\"},{\"id\":\"1731464f-3304-4ee2-9fc4-09dab87fd367\",\"type\":\"BoxZoomTool\"},{\"id\":\"494f14da-0495-47ea-9733-6012d9679584\",\"type\":\"SaveTool\"},{\"id\":\"ce42fc11-29f6-486e-869e-3b72d8ee2a2c\",\"type\":\"ResetTool\"},{\"id\":\"2c38e1c4-34ba-4ae4-89c1-2d31b1a9f872\",\"type\":\"HelpTool\"}]},\"id\":\"152d1e46-4a54-41be-96d8-cb1c4fef19d4\",\"type\":\"ProxyToolbar\"},{\"attributes\":{\"callback\":null,\"start\":0},\"id\":\"1b497666-1ebc-468a-b5a9-e71cf81d9237\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"l\",\"counts\",\"x\"],\"data\":{\"counts\":[1000,1000],\"l\":[\"pd_count\",\"pd_count\"],\"x\":[[\"alt.atheism\",\"pd_count\"],[\"comp.graphics\",\"pd_count\"]]}},\"id\":\"9d831602-ee07-4fea-b997-13e71889109b\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"grid_line_color\":{\"value\":null},\"plot\":{\"id\":\"b96d4bef-177e-4662-a4d6-018226782c9b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"9cba2bbb-2b83-40b4-a364-9392411ed894\",\"type\":\"CategoricalTicker\"}},\"id\":\"52705e28-21cd-4303-ac46-0846aed55778\",\"type\":\"Grid\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"ad5dbfd9-34e7-4f0f-8efd-24dde6aeede2\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"25f0ea90-2da2-4c9a-a70e-8c5749df5010\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"2f001676-4661-43e8-afe2-b84120c8ccc1\",\"type\":\"CategoricalScale\"},{\"attributes\":{},\"id\":\"ce42fc11-29f6-486e-869e-3b72d8ee2a2c\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"217574b9-2208-4391-b631-27d39be87bde\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"below\":[{\"id\":\"3b719cf9-ea99-42ed-b30f-04ad25208b01\",\"type\":\"CategoricalAxis\"}],\"left\":[{\"id\":\"8a5c5364-8aa5-4d2b-9f26-ed4857bef0c6\",\"type\":\"LinearAxis\"}],\"outline_line_color\":{\"value\":null},\"plot_height\":495,\"plot_width\":981,\"renderers\":[{\"id\":\"3b719cf9-ea99-42ed-b30f-04ad25208b01\",\"type\":\"CategoricalAxis\"},{\"id\":\"52705e28-21cd-4303-ac46-0846aed55778\",\"type\":\"Grid\"},{\"id\":\"8a5c5364-8aa5-4d2b-9f26-ed4857bef0c6\",\"type\":\"LinearAxis\"},{\"id\":\"30fe2cba-b678-4cd1-80e8-d54ccc4c8490\",\"type\":\"Grid\"},{\"id\":\"ad5dbfd9-34e7-4f0f-8efd-24dde6aeede2\",\"type\":\"BoxAnnotation\"},{\"id\":\"908617d8-7332-47d6-ad06-3a089a501302\",\"type\":\"Legend\"},{\"id\":\"7bfc5451-a4fc-4e80-b062-e6050d039949\",\"type\":\"GlyphRenderer\"}],\"title\":null,\"toolbar\":{\"id\":\"1a712119-f61f-438c-9564-754b5c6d7889\",\"type\":\"Toolbar\"},\"toolbar_location\":null,\"x_range\":{\"id\":\"6fc68874-5861-4f3e-9f59-534c6be28ba4\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"2f001676-4661-43e8-afe2-b84120c8ccc1\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"1b497666-1ebc-468a-b5a9-e71cf81d9237\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"34670782-84d9-4e1b-b4d3-cda811d7e4ea\",\"type\":\"LinearScale\"}},\"id\":\"b96d4bef-177e-4662-a4d6-018226782c9b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"callback\":null,\"factors\":[[\"alt.atheism\",\"pd_count\"],[\"comp.graphics\",\"pd_count\"]],\"range_padding\":0.1},\"id\":\"6fc68874-5861-4f3e-9f59-534c6be28ba4\",\"type\":\"FactorRange\"},{\"attributes\":{\"children\":[{\"id\":\"30ae4915-0921-48e1-8e08-70a0828ae80a\",\"type\":\"ToolbarBox\"},{\"id\":\"12d44aaa-4707-4409-ba9f-13597e9fecef\",\"type\":\"Column\"}]},\"id\":\"79545c21-181a-4cfe-a5b7-a75354205411\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"ab744692-ed5a-4b63-b53f-53f4e24521cf\",\"type\":\"BasicTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"9d831602-ee07-4fea-b997-13e71889109b\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"9a3e4611-ecaa-460d-b8f9-c814aedb9334\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"36cdd6ae-3c18-4cf5-81df-79fd12269e67\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"87e92aed-3760-4afb-9c54-4197e9ba9daa\",\"type\":\"CDSView\"}},\"id\":\"7bfc5451-a4fc-4e80-b062-e6050d039949\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"children\":[{\"id\":\"5249f22f-0d93-4bed-aa7e-03fae3ceee6d\",\"type\":\"Row\"}]},\"id\":\"12d44aaa-4707-4409-ba9f-13597e9fecef\",\"type\":\"Column\"},{\"attributes\":{\"children\":[{\"id\":\"b96d4bef-177e-4662-a4d6-018226782c9b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}]},\"id\":\"5249f22f-0d93-4bed-aa7e-03fae3ceee6d\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"494f14da-0495-47ea-9733-6012d9679584\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"9cba2bbb-2b83-40b4-a364-9392411ed894\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"b96d4bef-177e-4662-a4d6-018226782c9b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"ab744692-ed5a-4b63-b53f-53f4e24521cf\",\"type\":\"BasicTicker\"}},\"id\":\"30fe2cba-b678-4cd1-80e8-d54ccc4c8490\",\"type\":\"Grid\"},{\"attributes\":{\"end\":2,\"factors\":[\"pd_count\"],\"palette\":[\"#1f77b4\"],\"start\":1},\"id\":\"c95c36a1-2a66-4b54-85f8-1756606044d0\",\"type\":\"CategoricalColorMapper\"},{\"attributes\":{},\"id\":\"215e9e3f-5dba-49e4-af70-2368cd373e80\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"34670782-84d9-4e1b-b4d3-cda811d7e4ea\",\"type\":\"LinearScale\"}],\"root_ids\":[\"79545c21-181a-4cfe-a5b7-a75354205411\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.13\"}}';\n",
       "          var render_items = [{\"docid\":\"8b8f9dae-f2f0-4721-9037-f3a16f3ee289\",\"elementid\":\"a19914f7-a322-421a-bc90-3bb23b08be55\",\"modelid\":\"79545c21-181a-4cfe-a5b7-a75354205411\"}];\n",
       "          root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        \n",
       "          }\n",
       "          if (root.Bokeh !== undefined) {\n",
       "            embed_document(root);\n",
       "          } else {\n",
       "            var attempts = 0;\n",
       "            var timer = setInterval(function(root) {\n",
       "              if (root.Bokeh !== undefined) {\n",
       "                embed_document(root);\n",
       "                clearInterval(timer);\n",
       "              }\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "                clearInterval(timer);\n",
       "              }\n",
       "            }, 10, root)\n",
       "          }\n",
       "        })(window);\n",
       "      });\n",
       "    };\n",
       "    if (document.readyState != \"loading\") fn();\n",
       "    else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "  })();\n",
       "<\\/script>`\n",
       "                                var chartscript = el.childNodes[1]\n",
       "                                var s = document.createElement(\"script\")\n",
       "                                s.innerHTML = chartscript.innerHTML\n",
       "                                d.parentNode.insertBefore(s, d)\n",
       "                            }\n",
       "                        }\n",
       "                    }\n",
       "                    if (!window.Bokeh && !window.autoload){\n",
       "                        window.autoload=true;\n",
       "                        \n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = 1;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  \n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.13.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));\n",
       "                    }\n",
       "                    setChartScript()\n",
       "                    </script>\n",
       "                    <script>\n",
       "  (function() {\n",
       "    var fn = function() {\n",
       "      Bokeh.safely(function() {\n",
       "        (function(root) {\n",
       "          function embed_document(root) {\n",
       "            \n",
       "          var docs_json = '{\"8b8f9dae-f2f0-4721-9037-f3a16f3ee289\":{\"roots\":{\"references\":[{\"attributes\":{\"source\":{\"id\":\"9d831602-ee07-4fea-b997-13e71889109b\",\"type\":\"ColumnDataSource\"}},\"id\":\"87e92aed-3760-4afb-9c54-4197e9ba9daa\",\"type\":\"CDSView\"},{\"attributes\":{\"axis_label\":\"topic\",\"formatter\":{\"id\":\"08868477-0e87-493d-90b0-a4bcac196dbc\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":1,\"major_label_text_color\":{\"value\":null},\"major_label_text_font_size\":{\"value\":\"0px\"},\"major_tick_line_color\":{\"value\":null},\"minor_tick_line_color\":{\"value\":null},\"plot\":{\"id\":\"b96d4bef-177e-4662-a4d6-018226782c9b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"9cba2bbb-2b83-40b4-a364-9392411ed894\",\"type\":\"CategoricalTicker\"}},\"id\":\"3b719cf9-ea99-42ed-b30f-04ad25208b01\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"2c38e1c4-34ba-4ae4-89c1-2d31b1a9f872\",\"type\":\"HelpTool\"},{\"attributes\":{\"toolbar\":{\"id\":\"152d1e46-4a54-41be-96d8-cb1c4fef19d4\",\"type\":\"ProxyToolbar\"},\"toolbar_location\":\"above\"},\"id\":\"30ae4915-0921-48e1-8e08-70a0828ae80a\",\"type\":\"ToolbarBox\"},{\"attributes\":{\"axis_label\":\"pd_count\",\"formatter\":{\"id\":\"215e9e3f-5dba-49e4-af70-2368cd373e80\",\"type\":\"BasicTickFormatter\"},\"minor_tick_line_color\":{\"value\":null},\"plot\":{\"id\":\"b96d4bef-177e-4662-a4d6-018226782c9b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"ab744692-ed5a-4b63-b53f-53f4e24521cf\",\"type\":\"BasicTicker\"}},\"id\":\"8a5c5364-8aa5-4d2b-9f26-ed4857bef0c6\",\"type\":\"LinearAxis\"},{\"attributes\":{\"items\":[{\"id\":\"7baff1fc-0ac8-4f21-8b34-944d86436ffa\",\"type\":\"LegendItem\"}],\"location\":\"top_left\",\"plot\":{\"id\":\"b96d4bef-177e-4662-a4d6-018226782c9b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"908617d8-7332-47d6-ad06-3a089a501302\",\"type\":\"Legend\"},{\"attributes\":{\"fill_color\":{\"field\":\"x\",\"transform\":{\"id\":\"c95c36a1-2a66-4b54-85f8-1756606044d0\",\"type\":\"CategoricalColorMapper\"}},\"line_color\":{\"field\":\"x\",\"transform\":{\"id\":\"c95c36a1-2a66-4b54-85f8-1756606044d0\",\"type\":\"CategoricalColorMapper\"}},\"top\":{\"field\":\"counts\"},\"width\":{\"value\":0.925},\"x\":{\"field\":\"x\"}},\"id\":\"9a3e4611-ecaa-460d-b8f9-c814aedb9334\",\"type\":\"VBar\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"25f0ea90-2da2-4c9a-a70e-8c5749df5010\",\"type\":\"PanTool\"},{\"id\":\"217574b9-2208-4391-b631-27d39be87bde\",\"type\":\"WheelZoomTool\"},{\"id\":\"1731464f-3304-4ee2-9fc4-09dab87fd367\",\"type\":\"BoxZoomTool\"},{\"id\":\"494f14da-0495-47ea-9733-6012d9679584\",\"type\":\"SaveTool\"},{\"id\":\"ce42fc11-29f6-486e-869e-3b72d8ee2a2c\",\"type\":\"ResetTool\"},{\"id\":\"2c38e1c4-34ba-4ae4-89c1-2d31b1a9f872\",\"type\":\"HelpTool\"}]},\"id\":\"1a712119-f61f-438c-9564-754b5c6d7889\",\"type\":\"Toolbar\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"counts\"},\"width\":{\"value\":0.925},\"x\":{\"field\":\"x\"}},\"id\":\"36cdd6ae-3c18-4cf5-81df-79fd12269e67\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"08868477-0e87-493d-90b0-a4bcac196dbc\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"label\":{\"field\":\"l\"},\"renderers\":[{\"id\":\"7bfc5451-a4fc-4e80-b062-e6050d039949\",\"type\":\"GlyphRenderer\"}]},\"id\":\"7baff1fc-0ac8-4f21-8b34-944d86436ffa\",\"type\":\"LegendItem\"},{\"attributes\":{\"overlay\":{\"id\":\"ad5dbfd9-34e7-4f0f-8efd-24dde6aeede2\",\"type\":\"BoxAnnotation\"}},\"id\":\"1731464f-3304-4ee2-9fc4-09dab87fd367\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"tools\":[{\"id\":\"25f0ea90-2da2-4c9a-a70e-8c5749df5010\",\"type\":\"PanTool\"},{\"id\":\"217574b9-2208-4391-b631-27d39be87bde\",\"type\":\"WheelZoomTool\"},{\"id\":\"1731464f-3304-4ee2-9fc4-09dab87fd367\",\"type\":\"BoxZoomTool\"},{\"id\":\"494f14da-0495-47ea-9733-6012d9679584\",\"type\":\"SaveTool\"},{\"id\":\"ce42fc11-29f6-486e-869e-3b72d8ee2a2c\",\"type\":\"ResetTool\"},{\"id\":\"2c38e1c4-34ba-4ae4-89c1-2d31b1a9f872\",\"type\":\"HelpTool\"}]},\"id\":\"152d1e46-4a54-41be-96d8-cb1c4fef19d4\",\"type\":\"ProxyToolbar\"},{\"attributes\":{\"callback\":null,\"start\":0},\"id\":\"1b497666-1ebc-468a-b5a9-e71cf81d9237\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"l\",\"counts\",\"x\"],\"data\":{\"counts\":[1000,1000],\"l\":[\"pd_count\",\"pd_count\"],\"x\":[[\"alt.atheism\",\"pd_count\"],[\"comp.graphics\",\"pd_count\"]]}},\"id\":\"9d831602-ee07-4fea-b997-13e71889109b\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"grid_line_color\":{\"value\":null},\"plot\":{\"id\":\"b96d4bef-177e-4662-a4d6-018226782c9b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"9cba2bbb-2b83-40b4-a364-9392411ed894\",\"type\":\"CategoricalTicker\"}},\"id\":\"52705e28-21cd-4303-ac46-0846aed55778\",\"type\":\"Grid\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"ad5dbfd9-34e7-4f0f-8efd-24dde6aeede2\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"25f0ea90-2da2-4c9a-a70e-8c5749df5010\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"2f001676-4661-43e8-afe2-b84120c8ccc1\",\"type\":\"CategoricalScale\"},{\"attributes\":{},\"id\":\"ce42fc11-29f6-486e-869e-3b72d8ee2a2c\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"217574b9-2208-4391-b631-27d39be87bde\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"below\":[{\"id\":\"3b719cf9-ea99-42ed-b30f-04ad25208b01\",\"type\":\"CategoricalAxis\"}],\"left\":[{\"id\":\"8a5c5364-8aa5-4d2b-9f26-ed4857bef0c6\",\"type\":\"LinearAxis\"}],\"outline_line_color\":{\"value\":null},\"plot_height\":495,\"plot_width\":981,\"renderers\":[{\"id\":\"3b719cf9-ea99-42ed-b30f-04ad25208b01\",\"type\":\"CategoricalAxis\"},{\"id\":\"52705e28-21cd-4303-ac46-0846aed55778\",\"type\":\"Grid\"},{\"id\":\"8a5c5364-8aa5-4d2b-9f26-ed4857bef0c6\",\"type\":\"LinearAxis\"},{\"id\":\"30fe2cba-b678-4cd1-80e8-d54ccc4c8490\",\"type\":\"Grid\"},{\"id\":\"ad5dbfd9-34e7-4f0f-8efd-24dde6aeede2\",\"type\":\"BoxAnnotation\"},{\"id\":\"908617d8-7332-47d6-ad06-3a089a501302\",\"type\":\"Legend\"},{\"id\":\"7bfc5451-a4fc-4e80-b062-e6050d039949\",\"type\":\"GlyphRenderer\"}],\"title\":null,\"toolbar\":{\"id\":\"1a712119-f61f-438c-9564-754b5c6d7889\",\"type\":\"Toolbar\"},\"toolbar_location\":null,\"x_range\":{\"id\":\"6fc68874-5861-4f3e-9f59-534c6be28ba4\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"2f001676-4661-43e8-afe2-b84120c8ccc1\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"1b497666-1ebc-468a-b5a9-e71cf81d9237\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"34670782-84d9-4e1b-b4d3-cda811d7e4ea\",\"type\":\"LinearScale\"}},\"id\":\"b96d4bef-177e-4662-a4d6-018226782c9b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"callback\":null,\"factors\":[[\"alt.atheism\",\"pd_count\"],[\"comp.graphics\",\"pd_count\"]],\"range_padding\":0.1},\"id\":\"6fc68874-5861-4f3e-9f59-534c6be28ba4\",\"type\":\"FactorRange\"},{\"attributes\":{\"children\":[{\"id\":\"30ae4915-0921-48e1-8e08-70a0828ae80a\",\"type\":\"ToolbarBox\"},{\"id\":\"12d44aaa-4707-4409-ba9f-13597e9fecef\",\"type\":\"Column\"}]},\"id\":\"79545c21-181a-4cfe-a5b7-a75354205411\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"ab744692-ed5a-4b63-b53f-53f4e24521cf\",\"type\":\"BasicTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"9d831602-ee07-4fea-b997-13e71889109b\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"9a3e4611-ecaa-460d-b8f9-c814aedb9334\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"36cdd6ae-3c18-4cf5-81df-79fd12269e67\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"87e92aed-3760-4afb-9c54-4197e9ba9daa\",\"type\":\"CDSView\"}},\"id\":\"7bfc5451-a4fc-4e80-b062-e6050d039949\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"children\":[{\"id\":\"5249f22f-0d93-4bed-aa7e-03fae3ceee6d\",\"type\":\"Row\"}]},\"id\":\"12d44aaa-4707-4409-ba9f-13597e9fecef\",\"type\":\"Column\"},{\"attributes\":{\"children\":[{\"id\":\"b96d4bef-177e-4662-a4d6-018226782c9b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}]},\"id\":\"5249f22f-0d93-4bed-aa7e-03fae3ceee6d\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"494f14da-0495-47ea-9733-6012d9679584\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"9cba2bbb-2b83-40b4-a364-9392411ed894\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"b96d4bef-177e-4662-a4d6-018226782c9b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"ab744692-ed5a-4b63-b53f-53f4e24521cf\",\"type\":\"BasicTicker\"}},\"id\":\"30fe2cba-b678-4cd1-80e8-d54ccc4c8490\",\"type\":\"Grid\"},{\"attributes\":{\"end\":2,\"factors\":[\"pd_count\"],\"palette\":[\"#1f77b4\"],\"start\":1},\"id\":\"c95c36a1-2a66-4b54-85f8-1756606044d0\",\"type\":\"CategoricalColorMapper\"},{\"attributes\":{},\"id\":\"215e9e3f-5dba-49e4-af70-2368cd373e80\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"34670782-84d9-4e1b-b4d3-cda811d7e4ea\",\"type\":\"LinearScale\"}],\"root_ids\":[\"79545c21-181a-4cfe-a5b7-a75354205411\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.13\"}}';\n",
       "          var render_items = [{\"docid\":\"8b8f9dae-f2f0-4721-9037-f3a16f3ee289\",\"elementid\":\"a19914f7-a322-421a-bc90-3bb23b08be55\",\"modelid\":\"79545c21-181a-4cfe-a5b7-a75354205411\"}];\n",
       "          root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        \n",
       "          }\n",
       "          if (root.Bokeh !== undefined) {\n",
       "            embed_document(root);\n",
       "          } else {\n",
       "            var attempts = 0;\n",
       "            var timer = setInterval(function(root) {\n",
       "              if (root.Bokeh !== undefined) {\n",
       "                embed_document(root);\n",
       "                clearInterval(timer);\n",
       "              }\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "                clearInterval(timer);\n",
       "              }\n",
       "            }, 10, root)\n",
       "          }\n",
       "        })(window);\n",
       "      });\n",
       "    };\n",
       "    if (document.readyState != \"loading\") fn();\n",
       "    else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "  })();\n",
       "</script><div style=\"padding:5px\" id=\"pd-bkchartdiv-beed0992\">\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"a19914f7-a322-421a-bc90-3bb23b08be55\"></div>\n",
       "</div></div>\n",
       "                    \n",
       "                \n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pixiedust\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "# Here we are only interested in the topic and text\n",
    "schemaString = \"topic text\"\n",
    "\n",
    "# A StructField object comprises three fields, name (a string), dataType (a DataType) and nullable (a bool). \n",
    "# We create 2 fields of strings with names according to our schemaString\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "# these together define our schema\n",
    "schema = StructType(fields)\n",
    "\n",
    "print(fnt_RDD.take(1))\n",
    "# Apply the schema in createDataFrame, to create a DataFrame 'df' from the RDD\n",
    "df = sqlContext.createDataFrame(fnt_RDD, schema)\n",
    "\n",
    "#print the schema of our DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "#Use pixiedust to show the number of topics by frequency\n",
    "#there are only 2 topics here, so lets see them\n",
    "display(df.select('topic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        topic|\n",
      "+-------------+\n",
      "|comp.graphics|\n",
      "|  alt.atheism|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a (temporary) view using the DataFrame, so that we can us use SparkSQL.\n",
    "df.createOrReplaceTempView(\"newsgroups\")\n",
    "\n",
    "# SQL can now be run on the DataFrame. \n",
    "# Let's start by selecting only the topics elements of each row \n",
    "results = sqlContext.sql(\"SELECT DISTINCT topic FROM newsgroups\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|        topic| cnt|\n",
      "+-------------+----+\n",
      "|comp.graphics|1000|\n",
      "|  alt.atheism|1000|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can make more sophisticated queries in SQL, e.g. using  topic names as a distinct feature and simply count number of files\n",
    "results_topic = sqlContext.sql(\"SELECT DISTINCT topic, count(*) as cnt FROM newsgroups GROUP BY topic ORDER BY cnt DESC\")\n",
    "results_topic.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need numeric labels for the classifier, for now we go for binary labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----+\n",
      "|      topic|                text|label|\n",
      "+-----------+--------------------+-----+\n",
      "|alt.atheism|Path: cantaloupe....|  0.0|\n",
      "|alt.atheism|Newsgroups: alt.a...|  0.0|\n",
      "|alt.atheism|Newsgroups: alt.a...|  0.0|\n",
      "+-----------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------------+--------------------+-----+\n",
      "|        topic|                text|label|\n",
      "+-------------+--------------------+-----+\n",
      "|comp.graphics|Newsgroups: comp....|  1.0|\n",
      "|comp.graphics|Xref: cantaloupe....|  1.0|\n",
      "|comp.graphics|Newsgroups: comp....|  1.0|\n",
      "+-------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.withColumn Returns a new DataFrame by adding a column with a name and value for each row.\n",
    "# The value is a 'column expression', where we compares with the string 'comp', to find out whether the topic is about computing.\n",
    "# double - will convert the resulting Boolean value into a number\n",
    "news_Groups = df.withColumn(\"label\",df.topic.like(\"comp%\").cast(\"double\"))\n",
    "alt_topic_df = news_Groups[df.topic.like(\"alt%\")]\n",
    "alt_topic_df.show(3)\n",
    "comp_topic_df = news_Groups[df.topic.like(\"comp%\")]\n",
    "comp_topic_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total document count: 2000\n",
      "Training-set count: 1191\n",
      "Test-set count: 809\n"
     ]
    }
   ],
   "source": [
    "#Create the training and testing set from the dataframe above\n",
    "#randomSplit - splits the Df into training/testing using the weights \n",
    "train_set, test_set = news_Groups.randomSplit([0.6, 0.4], 123)\n",
    "print (\"Total document count:\",news_Groups.count())\n",
    "print (\"Training-set count:\",train_set.count())\n",
    "print (\"Test-set count:\",test_set.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Using ML to classify messages \n",
    "\n",
    "ML is the Spark machine learning library for DataFrames. We want to build an ML pipeline to predict the Binary label.\n",
    "\n",
    "A Spark ML Pipeline is a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage.\n",
    "\n",
    "A practical ML pipeline might consist of many stages like feature extraction, feature transformation, and model fitting. We use  pipeline that consists of the following stages:\n",
    "\n",
    "    a)RegexTokenizer - which tokenizes each article into a sequence of words with a regex pattern,\n",
    "    b)HashingTF, which maps the word sequences produced by RegexTokenizer to sparse feature vectors using the hashing trick,\n",
    "    c)LogisticRegression, which fits the feature vectors and the labels from the training data to a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF,StopWordsRemover,IDF,Tokenizer\n",
    "\n",
    "#Constructing a pipeline\n",
    "#We split each sentence into words using Tokenizer. \n",
    "#Tokenizer only splits by white spaces\n",
    "tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "\n",
    "#Remove stopwords\n",
    "remover= StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(False)\n",
    "\n",
    "#For each sentence (bag of words),use HashingTF to hash the sentence into a feature vector. \n",
    "hashingTF = HashingTF().setNumFeatures(1000).setInputCol(\"filtered\").setOutputCol(\"rawFeatures\")\n",
    "\n",
    "#We use IDF to rescale the feature vectors; this generally improves performance when using text as features.\n",
    "idf = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)\n",
    "\n",
    "#Our feature vectors could then be passed to a learning algorithm.\n",
    "lr = LogisticRegression()\n",
    "#nb = NaiveBayes()\n",
    "\n",
    "#Then basically we connect all the steps above to create one pipeline\n",
    "pipeline=Pipeline(stages=[tokenizer,remover,hashingTF,idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: inputCol: input column name. (current: text)\n",
      "outputCol: output column name. (default: Tokenizer_4c97bf77fce3b9bbe336__output, current: words)\n",
      "\n",
      "\n",
      "\n",
      "Remover: caseSensitive: whether to do a case sensitive comparison over the stop words (default: False, current: False)\n",
      "inputCol: input column name. (current: words)\n",
      "outputCol: output column name. (default: StopWordsRemover_47e2b60565ef6eafa048__output, current: filtered)\n",
      "stopWords: The words to be filtered out (default: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'])\n",
      "\n",
      "\n",
      "\n",
      "HashingTF: binary: If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default False. (default: False)\n",
      "inputCol: input column name. (current: filtered)\n",
      "numFeatures: number of features. (default: 262144, current: 1000)\n",
      "outputCol: output column name. (default: HashingTF_4bd98cd0776633df91ae__output, current: rawFeatures)\n",
      "\n",
      "\n",
      "\n",
      "IDF: inputCol: input column name. (current: rawFeatures)\n",
      "minDocFreq: minimum number of documents in which a term should appear for filtering (default: 0, current: 0)\n",
      "outputCol: output column name. (default: IDF_46a199cbbc281eec93bb__output, current: features)\n",
      "\n",
      "\n",
      "\n",
      "Pipeline: stages: a list of pipeline stages (current: [Tokenizer_4c97bf77fce3b9bbe336, StopWordsRemover_47e2b60565ef6eafa048, HashingTF_4bd98cd0776633df91ae, IDF_46a199cbbc281eec93bb, LogisticRegression_4c7f8469c00f0f0d3a78])\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We can get an information for each parameter  using the .explainParams()\n",
    "print (\"Tokenizer:\",tokenizer.explainParams())\n",
    "print(\"\\n\\n\")\n",
    "print (\"Remover:\",remover.explainParams())\n",
    "print(\"\\n\\n\")\n",
    "print (\"HashingTF:\",hashingTF.explainParams())\n",
    "print (\"\\n\\n\")\n",
    "print (\"IDF:\",idf.explainParams())\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print (\"Pipeline:\",pipeline.explainParams())\n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 4 µs, total: 10 µs\n",
      "Wall time: 19.3 µs\n"
     ]
    }
   ],
   "source": [
    "#Use the pipeline option to fit the training set and create a model\n",
    "\n",
    "# Time\n",
    "%time\n",
    "\n",
    "# After we construct this ML pipeline,we can fit it to the training data\n",
    "# and obtain a trained pipeline model that can be used for prediction.\n",
    "model=pipeline.fit(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Evaluate prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+-----+\n",
      "|      topic|         probability|prediction|label|\n",
      "+-----------+--------------------+----------+-----+\n",
      "|alt.atheism|[0.99999999842345...|       0.0|  0.0|\n",
      "|alt.atheism|[1.0,5.4529774409...|       0.0|  0.0|\n",
      "|alt.atheism|[0.99999903043298...|       0.0|  0.0|\n",
      "|alt.atheism|[0.99999993335625...|       0.0|  0.0|\n",
      "|alt.atheism|[0.99999999876034...|       0.0|  0.0|\n",
      "+-----------+--------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------+--------------------+----------+-----+\n",
      "|        topic|         probability|prediction|label|\n",
      "+-------------+--------------------+----------+-----+\n",
      "|comp.graphics|[3.81519421387831...|       1.0|  1.0|\n",
      "|comp.graphics|[2.62279418030592...|       1.0|  1.0|\n",
      "|comp.graphics|[3.28786544252066...|       1.0|  1.0|\n",
      "|comp.graphics|[3.9573191656368E...|       1.0|  1.0|\n",
      "|comp.graphics|[2.80388447773842...|       1.0|  1.0|\n",
      "+-------------+--------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# After we obtain a fitted pipeline model, we want to know how well it performs. \n",
    "# Let us start with some manual checks by displaying the predicted labels.\n",
    "test_predictions = model.transform(test_set)\n",
    "train_predictions = model.transform(train_set)\n",
    "\n",
    "# Show the predicted labels along with true labels and raw texts.\n",
    "test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").show(5)\n",
    "# and show some of the other class ...\n",
    "test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").filter(test_predictions.topic.like(\"comp%\")).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC curve - training: 1.0\n",
      "Area under ROC curve - testing: 0.9960883542973084\n"
     ]
    }
   ],
   "source": [
    "# The predicted labels look accurate. \n",
    "# Let's evaluate the model quantitatively.\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "print (\"Area under ROC curve - training:\",evaluator.evaluate(train_predictions))\n",
    "print (\"Area under ROC curve - testing:\",evaluator.evaluate(test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training result is already perfect, the test result is also excellent. So, this task is easy for LogisticRegression. With 20 classes, the task gets harder however. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification\n",
    "\n",
    "Use all the 20 toics in the dataset as class labels. The reading of the data is straightforward. We will need to use a different mapping from newsgroup names to class labels, though. Then we'll need a different evaluation, as the ROC AUC is only defined for the binary case. \n",
    "\n",
    "The performance will be lower, so that it is worth to try and tune the hyper-parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "\n",
    "#With 3 values for hashingTF.numFeatures and 3 values for idf,\n",
    "# this grid will have 3 x 3 = 9 parameter settings for CrossValidator to choose from.\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(hashingTF.numFeatures,[1000,10000,100000])\\\n",
    "    .addGrid(idf.minDocFreq,[0,10,100])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.64 s, sys: 1.39 s, total: 5.03 s\n",
      "Wall time: 13min 30s\n",
      "Area under the ROC curve for best fitted model = 0.9985820284327737\n"
     ]
    }
   ],
   "source": [
    "#A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(2)\n",
    "# Note: This takes a long time to run with proper \n",
    "# Do this step only when you've done everything else first!\n",
    "%time cvModel = cv.fit(train_set)\n",
    "print(\"Area under the ROC curve for best fitted model =\",evaluator.evaluate(cvModel.transform(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-bda7e21d2bee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Observe the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Area under ROC curve for non-tuned model:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Area under ROC curve for fitted model:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "print (\"Area under ROC curve for non-tuned model:\",evaluator.evaluate(predictions))\n",
    "print (\"Area under ROC curve for fitted model:\",evaluator.evaluate(cvModel.transform(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 \n",
    "\n",
    "\n",
    "\n",
    "## 7)Analysing XML Data\n",
    "\n",
    "http://ratings.food.gov.uk/open-data/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a)   Using the XML parser\n",
    "We start by reading the dataset into and parsing it using the ElementTree parser. \n",
    "The XML looks like this: \n",
    "\n",
    "`<FHRSEstablishment>\n",
    "    <Header>\n",
    "    <ExtractDate>2015-11-06</ExtractDate>\n",
    "    <ItemCount>1256</ItemCount>\n",
    "    <ReturnCode>Success</ReturnCode>\n",
    "    </Header>\n",
    "    <EstablishmentCollection>\n",
    "        <EstablishmentDetail>\n",
    "        <FHRSID>507136</FHRSID>\n",
    "        <LocalAuthorityBusinessID>PI/000081182</LocalAuthorityBusinessID>\n",
    "        <BusinessName>196</BusinessName>\n",
    "        <BusinessType>Restaurant/Cafe/Canteen</BusinessType>\n",
    "        <BusinessTypeID>1</BusinessTypeID>\n",
    "        <AddressLine1>Cambridge</AddressLine1>\n",
    "        <AddressLine2>Cambridgeshire</AddressLine2>\n",
    "        <PostCode>CB1 3NF</PostCode>\n",
    "        <RatingValue>5</RatingValue>\n",
    "        <RatingKey>fhrs_5_en-GB</RatingKey>\n",
    "        <RatingDate>2015-01-22</RatingDate>\n",
    "        <LocalAuthorityCode>027</LocalAuthorityCode>\n",
    "        <LocalAuthorityName>Cambridge City</LocalAuthorityName>\n",
    "            <LocalAuthorityWebSite>http://www.cambridge.gov.uk</LocalAuthorityWebSite>\n",
    "            <LocalAuthorityEmailAddress>env.health@cambridge.gov.uk</LocalAuthorityEmailAddress>\n",
    "        <Scores>\n",
    "            <Hygiene>5</Hygiene>\n",
    "            <Structural>0</Structural>\n",
    "            <ConfidenceInManagement>5</ConfidenceInManagement>\n",
    "        </Scores>\n",
    "        <SchemeType>FHRS</SchemeType>\n",
    "        <NewRatingPending>False</NewRatingPending>\n",
    "        <Geocode>\n",
    "            <Longitude>0.14503300000000</Longitude>\n",
    "            <Latitude>52.19734500000000</Latitude>\n",
    "        </Geocode>\n",
    "    </EstablishmentDetail>`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Get the XML Data\n",
    "\n",
    "\n",
    "The predefined `parseXML` function creates an XML parse tree. We'll start by creating an RDD that contains just the parse trees instead of text, then filter out the files that could not be parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work\n",
      "fatal: destination path 'City-Data-Science' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "#Get the data\n",
    "\n",
    "%cd ~/notebook/work/City-Data-Science/\n",
    "!git clone https://github.com/tweyde/City-Data-Science.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Counting objects: 6, done.\u001b[K\n",
      "remote: Total 6 (delta 2), reused 2 (delta 2), pack-reused 4\u001b[K\n",
      "Unpacking objects: 100% (6/6), done.\n",
      "From https://github.com/tweyde/City-Data-Science\n",
      "   ddd2162..5593edd  master     -> origin/master\n",
      "Updating ddd2162..5593edd\n",
      "Fast-forward\n",
      " datasets/foodhygiene.zip | Bin 0 -> 31777575 bytes\n",
      " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
      " create mode 100644 datasets/foodhygiene.zip\n",
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/datasets\n",
      "\u001b[0m\u001b[01;34m20_newsgroups\u001b[0m/        20_newsgroups.tar.gz.1  \u001b[01;34mlingspam_public\u001b[0m/\n",
      "20_newsgroups.tar.gz  foodhygiene.zip         lingspam_public02.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!git pull\n",
    "%cd datasets/\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m20_newsgroups\u001b[0m/          \u001b[01;34mfoodhygiene\u001b[0m/      lingspam_public02.tar.gz\r\n",
      "20_newsgroups.tar.gz    foodhygiene.zip\r\n",
      "20_newsgroups.tar.gz.1  \u001b[01;34mlingspam_public\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "!rm -R foodhygiene\n",
    "!unzip -q foodhygiene.zip\n",
    "%ls\n",
    "!echo \"datasets/foodhygiene/**\" >> ~/notebook/work/City-Data-Science/.git/info/exclude\n",
    "# add the newly created directory to the list of excluded dirs to prevent accidental uploading to github\n",
    "# do this only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771\n",
      "[('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/datasets/foodhygiene/FHRS524en-GB.xml', <Element 'FHRSEstablishment' at 0x7f52b5fd2f48>)]\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parseXML(f_x):\n",
    "    try:\n",
    "        root = ET.fromstring(f_x[1])\n",
    "    except ET.ParseError as err:\n",
    "        # parsing error :-(\n",
    "        root = None\n",
    "    return (f_x[0], root)\n",
    "\n",
    "%cd ~\n",
    "p = os.path.abspath('./notebook/work/City-Data-Science/datasets/foodhygiene')\n",
    "rawData = sc.wholeTextFiles(p)\n",
    "parsedData = rawData.map(lambda f_x: parseXML(f_x)) #<<<< map to XML parse trees\n",
    "parsedData = parsedData.filter(lambda f_et: f_et[1] is not None) #<<< filter out items where the parse tree is `None`\n",
    "print(parsedData.take(1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Find establishements with valid hygiene ratings\n",
    "\n",
    "Find all the possible RatingValue elements. For that we use the ElementTree function `findall`. We use a syntax called XPath, which enables us to find elements lower down in the tree without explicitly traversing it. The XPath syntax ist shown here: [https://docs.python.org/3.5/library/xml.etree.elementtree.html?highlight=elementtree#supported-xpath-syntax](https://docs.python.org/3.5/library/xml.etree.elementtree.html?highlight=elementtree#supported-xpath-syntax)\n",
    "\n",
    "For finding the rating values, use `element.findall('.//RatingValue')` where element shoudl be the root of a parse tree. This gives a list of XML elements `x`, where we are interested in their `x.text` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4',\n",
       " '1',\n",
       " 'Pass',\n",
       " 'Pass and Eat Safe',\n",
       " 'Awaiting Inspection',\n",
       " '0',\n",
       " 'AwaitingPublication',\n",
       " 'Improvement Required',\n",
       " 'AwaitingInspection',\n",
       " 'Exempt',\n",
       " '5',\n",
       " '3',\n",
       " '2']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allRatings = parsedData.flatMap(lambda f_et: [x.text for x in f_et[1].findall('.//RatingValue')]).distinct() #<<< find distnct values\n",
    "allRatings.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Remove non-numeric ratings\n",
    "\n",
    "Now we want to get rid of the non-numeric ratings. These should be:\n",
    "\n",
    "    invalidRatings = ['Pass','Pass and Eat Safe','Awaiting Inspection','AwaitingPublication','Improvement Required','AwaitingInspection','Exempt']\n",
    "    \n",
    "    \n",
    "First collect all XML elements tagged EstablishmentDetail (with finall as before), but keep the elements for further use (i.e. don't extract the text element as above).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497835\n",
      "411554\n"
     ]
    }
   ],
   "source": [
    "invalidRatings = ['Pass','Pass and Eat Safe','Awaiting Inspection','AwaitingPublication','Improvement Required','AwaitingInspection','Exempt']\n",
    "\n",
    "allEstData = parsedData.flatMap(lambda f_et: f_et[1].findall('.//EstablishmentDetail')) #<<< get the establishment detail\n",
    "print(allEstData.count()) # should be 497835\n",
    "\n",
    "estData = allEstData.filter(lambda est: est.find('RatingValue').text not in invalidRatings)\n",
    "print(estData.count()) # should be 411554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Find highest and lowest values\n",
    "\n",
    "Find the 10 local authorities with the highest and lowest mean hygiene rating. We now use find as there is only one local authority per establishment, which we use as key. We then get the RatingValue and average (not in reduce, so associativity is not required here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "import numpy as np\n",
    "authRatings = estData.map(lambda est: \n",
    "    (est.find('LocalAuthorityName').text, \n",
    "    [float(est.find('RatingValue').text)])).reduceByKey(add).map(lambda a_rl: (a_rl[0], np.mean(a_rl[1])))\n",
    "\n",
    "print(authRatings.sortBy(lambda x: x[1], ascending=False).take(10)) # using RDD.sortBy() get the 10 highest \n",
    "print(authRatings.sortBy(lambda x: x[1], ascending=True).take(10)) # using RDD.sortBy() get the 10 lowest \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Organise by PostCode\n",
    "Use the first part of the PostCode node (i.e. for IP7 5BY, only use IP7) to find the best and worse postcodes for food hygiene.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('WF2 7EQ', 5.0), ('HD1 6EE', 5.0), ('DH1 5TU', 5.0), ('EX5 4AS', 5.0), ('LL55 3LP', 5.0), ('SO14 0DA', 5.0), ('WA14 1RU', 5.0), ('BT56 8EW', 5.0), ('TN32 5XF', 5.0), ('WC1X 8LR', 5.0)]\n",
      "[('W1U 4AP', 0.0), ('BB5 4JT', 0.0), ('WF13 4DJ', 0.0), ('SK1 3EH', 0.0), ('BL9 7AY', 0.0), ('CH41 2UW', 0.0), ('SN2 1QR', 0.0), ('DT10 2BS', 0.0), ('SN5 5PD', 0.0), ('HU5 5JR', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "authRatings = estData.map(lambda est: \n",
    "    # organize by PostCode instead of LocalAuthority\n",
    "    (0 if est.find('PostCode') is None else est.find('PostCode').text, \n",
    "    [float(est.find('RatingValue').text)])).reduceByKey(add).map(lambda a_rl: (a_rl[0], np.mean(a_rl[1])))\n",
    "# output as above\n",
    "print(authRatings.sortBy(lambda x: x[1], ascending=False).take(10)) # using RDD.sortBy() get the 10 highest \n",
    "print(authRatings.sortBy(lambda x: x[1], ascending=True).take(10)) # using RDD.sortBy() get the 10 lowest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13) Organise by Type\n",
    "\n",
    "Use the BusinessType or BusinessTypeID nodes to discover and consolidate all business types. Find the 10 best and worse rated business types for the entire UK, per local authority and per postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('School/college/university', 4.808328845808652), ('Hospitals/Childcare/Caring Premises', 4.7188041594454075), ('Farmers/growers', 4.609657947686117), ('Other catering premises', 4.6071905775226405), ('Retailers - supermarkets/hypermarkets', 4.583096085409252), ('Hotel/bed & breakfast/guest house', 4.530760695351991), ('Mobile caterer', 4.482452707110242), ('Manufacturers/packers', 4.470830070477682), ('Distributors/Transporters', 4.455373406193078), ('Importers/Exporters', 4.311475409836065)]\n",
      "[('Takeaway/sandwich shop', 3.7914874651810586), ('Retailers - other', 4.155244145840947), ('Pub/bar/nightclub', 4.219805857601133), ('Restaurant/Cafe/Canteen', 4.247219035792088), ('Importers/Exporters', 4.311475409836065), ('Distributors/Transporters', 4.455373406193078), ('Manufacturers/packers', 4.470830070477682), ('Mobile caterer', 4.482452707110242), ('Hotel/bed & breakfast/guest house', 4.530760695351991), ('Retailers - supermarkets/hypermarkets', 4.583096085409252)]\n"
     ]
    }
   ],
   "source": [
    "authRatings = estData.map(lambda est: \n",
    "    # organize by BusinessType instead of LocalAuthority\n",
    "    (0 if est.find('BusinessType') is None else est.find('BusinessType').text, \n",
    "    [float(est.find('RatingValue').text)])).reduceByKey(add).map(lambda a_rl: (a_rl[0], np.mean(a_rl[1])))\n",
    "# ouput as above\n",
    "print(authRatings.sortBy(lambda x: x[1], ascending=False).take(10)) # using RDD.sortBy() get the 10 highest \n",
    "print(authRatings.sortBy(lambda x: x[1], ascending=True).take(10)) # using RDD.sortBy() get the 10 lowest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14) Web Scraping with an HTML parser\n",
    "\n",
    "This is the scraper that was used to get the food hygiene data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a code to scrape data from the web page.\n",
    "#This will take a *long* time\n",
    "\n",
    "import urllib.request\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MyHTMLParser(HTMLParser): # created a \n",
    "    links = None\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if self.links is None:\n",
    "            self.links = []\n",
    "        if tag == 'a':\n",
    "            href = None\n",
    "            for k,v in attrs: # keys and values of the tag attributes\n",
    "                if k == 'href': # if the key is 'href'\n",
    "                    href = v # we are interested in its value\n",
    "            if href is not None: # if there is a link\n",
    "                if href.endswith('en-GB.xml'): # and it is the type that we expect\n",
    "                    self.links.append(href) # then add to our list of links\n",
    "\n",
    "f = urllib.request.urlopen(\"http://ratings.food.gov.uk/open-data/en-GB\") # the ratings site\n",
    "\n",
    "parser = MyHTMLParser() # initiate our parser\n",
    "parser.feed(str(f.read())) # read from the URL\n",
    "for l in parser.links: # get the collected links\n",
    "    fname = l.split('/') # split them\n",
    "    fname = fname[-1]    # get filename\n",
    "    print('downloading %s' % fname) # print a message\n",
    "    urllib.request.urlretrieve(l, fname) # and get the file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
