{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling with n-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from operator import add\n",
    "\n",
    "\n",
    "def stripFinalS( word ):\n",
    "    word = word.lower() # lower case\n",
    "    if len(word) >0 and word[-1] == 's': # check for final letter\n",
    "        return word[:-1]\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "def splitFileWords(filenameContent):\n",
    "    f,c = filenameContent # split the input tuple  \n",
    "    fwLst = [] # the new list for (filename,word) tuples\n",
    "    wLst = re.split('\\W+',c) # <<< now create a word list wLst\n",
    "    for w in wLst: # iterate through the list\n",
    "        fwLst.append((f,stripFinalS(w))) # <<< and append (f,w) to the \n",
    "    return fwLst #return a list of (f,w) tuples \n",
    "\n",
    "def hashing_vectorizer(word_count_list, N):\n",
    "     v = [0] * N  # create fixed size vector of 0s\n",
    "     for word_count in word_count_list: \u000b",
    "         word,count = word_count \t# unpack tuple\n",
    "         h = hash(word) # get hash value\n",
    "         v[h % N] = v[h % N] + count # add count\n",
    "     return v # return hashed word vector\n",
    "\n",
    "def reGrpLst(fw_c): # we get a nested tuple\n",
    "    fw,c = fw_c\n",
    "    f,w = fw\n",
    "    return (f,[(w,c)]) # return (f,[(w,c)]) structure. \n",
    "\n",
    "N = 10\n",
    "\n",
    "#dirPath = 'hdfs://saltdean/data/library/'\n",
    "dirPath = '../../Data/library/'\n",
    "ft_RDD = sc.wholeTextFiles(dirPath) #<<< add code to create an RDD with wholeTextFiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Create N-grams and n-gram frequency vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'a b', 'a b c', 'b', 'b c', 'b c d', 'c', 'c d', 'c d e', 'd', 'd e', 'd e f', 'e', 'e f', 'f']\n",
      "['a', 'a b', 'a b c', 'b', 'b c', 'b c d', 'c', 'c d', 'c d e', 'd', 'd e', 'd e f', 'e', 'e f', 'f']\n",
      "[('file', 'a'), ('file', 'a b'), ('file', 'a b c'), ('file', 'b'), ('file', 'b c'), ('file', 'b c d'), ('file', 'c'), ('file', 'c d'), ('file', 'c d e'), ('file', 'd'), ('file', 'd e'), ('file', 'd e f'), ('file', 'e'), ('file', 'e f'), ('file', 'f')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('file:/Users/tweyde/Documents/CityUni/teaching/2017-18/Big Data/Data/library/emma.txt',\n",
       "  [40842, 36770, 26182, 32390, 31629, 31362, 29476, 42068, 23837, 35695]),\n",
       " ('file:/Users/tweyde/Documents/CityUni/teaching/2017-18/Big Data/Data/library/henry_V.txt',\n",
       "  [6283, 6095, 5555, 5891, 6033, 6137, 5027, 7704, 4921, 6455]),\n",
       " ('file:/Users/tweyde/Documents/CityUni/teaching/2017-18/Big Data/Data/library/king_lear.txt',\n",
       "  [7083, 6317, 5537, 6536, 6413, 5646, 5243, 7217, 5062, 5831])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Using a nested list comprehension to create n-grams given a string.\n",
    "def splitNGrams(text,n): # function for splitting a word list and creating n-grams\n",
    "    nGramLst = [] # the new list for (filename,word) tuples\n",
    "    wLst = re.split('\\W+', text) # now create a word list wLst\n",
    "    wLst = list(map(stripFinalS,wLst)) # remove final s from the word list (this is a local map, don't confuse with RDD or DF map)\n",
    "    wNum = len(wLst) # get total lenght to avoid overrunning at the end.\n",
    "    nGramLst = [' '.join(wLst[i:j]) for i in range(0,wNum) for j in range(i+1,min(wNum,i+n+1))]\n",
    "    return nGramLst #return a list of (f,w) tuples \n",
    "\n",
    "# Alternative version with separate function for converting a word-list to n-grams\n",
    "def lst2ngram(wLst,n):\n",
    "    wNum = len(wLst) # get total lenght to avoid overrunning at the end.\n",
    "    nGramLst = [] # output list\n",
    "    for i in range(0,wNum): # starting points \n",
    "        for j in range(i+1,min(wNum,i+n+1)): # end points of n-grams \n",
    "              nGramLst.append(' '.join(wLst[i:j])) # append to list\n",
    "    return nGramLst # done\n",
    "\n",
    "# a wrapper around the separate function\n",
    "def splitNGrams2(text,n): \n",
    "    wLst = re.split('\\W+', text) #  split into words\n",
    "    nGramLst = lst2ngram(wLst,n) # create the n-grams\n",
    "    return nGramLst # done\n",
    "\n",
    "# This function manages the filenames around the n-gram extraction  \n",
    "def splitFileNGrams(filenameContent,n=2): \n",
    "    f,c = filenameContent # split the input tuple \n",
    "    ngLst = splitNGrams(c,n) # split the file content into n-grams\n",
    "    fngLst = [] # the new list for (filename,n-gram) tuples \n",
    "    for ng in ngLst: # iterate through the list\n",
    "        fngLst.append((f,ng)) # and append (f,ng) to the \n",
    "    return fngLst #return a list of (f,ng) tuples \n",
    "\n",
    "# just for testing\n",
    "print(splitNGrams('a b c d e f g', 3)) # test the splitting function with s string (easier than with an RDD or DF)\n",
    "print(splitNGrams2('a b c d e f g', 3))# test the 2nd version, should look like the first\n",
    "print(splitFileNGrams(('file','a b c d e f g'), 3)) # should add the file tag before the n-grams\n",
    "\n",
    "# now let's use the new function to create RDDs with n-gram vectors\n",
    "from functools import partial\n",
    "# use a partial to define the max len of the n-grams\n",
    "fng_RDD = ft_RDD.flatMap(partial(splitFileNGrams,n=2)) \n",
    "fng_RDD.take(5)\n",
    "fng_1_RDD = fng_RDD.map(lambda x: (x,1))  # change (f,ng) to ((f,ng),1)\n",
    "fng_c_RDD = fng_1_RDD.reduceByKey(add) # add the ones\n",
    "f_ngcL_RDD = fng_c_RDD.map(reGrpLst) # regroup to (f,[(ng,c)])\n",
    "f_ngcL2_RDD = f_ngcL_RDD.reduceByKey(add) #<<< create [(w,c), ... ,(w,c)] lists per file\n",
    "f_ngVec_RDD = f_ngcL2_RDD.map(lambda f_wc: (f_wc[0],hashing_vectorizer(f_wc[1],N)))\n",
    "f_ngVec_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Convert to DataFrame\n",
    "\n",
    "The next task is create a DataFrame from the RDD. This is similar to what was shown in the lecture and also to the docuementation: [http://spark.apache.org/docs/2.0.0/sql-programming-guide.html#interoperating-with-rdds](http://spark.apache.org/docs/2.0.0/sql-programming-guide.html#interoperating-with-rdds)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library_DF.printSchema()\n",
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- vector: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "library_DF.show()\n",
      "+-----------+--------------------+\n",
      "|     author|              vector|\n",
      "+-----------+--------------------+\n",
      "|Shakespeare|[5486, 7575, 2808...|\n",
      "|Shakespeare|[863, 1502, 550, ...|\n",
      "|Shakespeare|[1119, 1299, 551,...|\n",
      "|Shakespeare|[620, 918, 389, 7...|\n",
      "|Shakespeare|[390, 954, 441, 6...|\n",
      "|Shakespeare|[2578, 3533, 1383...|\n",
      "|Shakespeare|[836, 1320, 618, ...|\n",
      "|Shakespeare|[4003, 5661, 2040...|\n",
      "|Shakespeare|[1333, 1622, 576,...|\n",
      "|Shakespeare|[700, 1047, 374, ...|\n",
      "|Shakespeare|[964, 1071, 400, ...|\n",
      "|Shakespeare|[5422, 8105, 2580...|\n",
      "|Shakespeare|[580, 1080, 536, ...|\n",
      "|Shakespeare|[2710, 4300, 1509...|\n",
      "|Shakespeare|[1340, 1516, 551,...|\n",
      "|Shakespeare|[1000, 1251, 424,...|\n",
      "|Shakespeare|[999, 1251, 424, ...|\n",
      "|Shakespeare|[4042, 5426, 2028...|\n",
      "|Shakespeare|[912, 1076, 462, ...|\n",
      "+-----------+--------------------+\n",
      "\n",
      "SELECT author,vector FROM library WHERE author=='Austen'\n",
      "+------+------+\n",
      "|author|vector|\n",
      "+------+------+\n",
      "+------+------+\n",
      "\n",
      "SELECT author,vector FROM library WHERE author!='Austen'\n",
      "[Row(author='Shakespeare', vector=[5486, 7575, 2808, 6961, 2045, 1747, 3145, 2501, 3600, 2949, 4448, 2845, 2743, 2237, 3852, 3419, 4367, 7739, 2493, 3958, 2428, 5567, 2669, 2691, 2584, 2215, 2092, 4632, 2081, 2916, 5731, 3069, 2387, 2245, 2815, 2305, 3253, 3147, 1962, 2878, 4804, 4142, 2794, 2915, 6717, 3024, 2126, 3974, 2677, 2931, 1864, 4013, 4780, 2099, 2729, 2489, 3051, 3440, 1953, 2140, 2373, 2475, 2099, 2173, 2943, 7326, 2142, 4773, 2061, 2422, 3844, 2104, 2297, 4169, 2261, 2018, 2409, 3927, 2451, 4870, 7157, 2649, 1901, 4584, 3155, 2495, 2520, 3853, 1782, 7591, 2707, 2331, 1704, 2316, 2528, 4324, 4371, 4082, 2777, 3040]), Row(author='Shakespeare', vector=[863, 1502, 550, 1071, 504, 480, 419, 519, 595, 555, 451, 396, 594, 416, 504, 624, 541, 1630, 682, 788, 533, 916, 699, 392, 577, 419, 534, 791, 495, 529, 492, 367, 430, 611, 546, 628, 575, 541, 542, 493, 615, 547, 534, 733, 1293, 492, 465, 938, 486, 634, 433, 566, 941, 408, 446, 540, 602, 647, 401, 676, 543, 427, 498, 536, 600, 1068, 366, 472, 408, 489, 525, 522, 439, 608, 512, 485, 512, 884, 441, 419, 1414, 414, 475, 660, 608, 515, 445, 553, 403, 1184, 414, 438, 395, 456, 443, 886, 568, 729, 468, 688]), Row(author='Shakespeare', vector=[1119, 1299, 551, 1118, 479, 387, 480, 546, 516, 553, 560, 457, 568, 418, 504, 646, 699, 1386, 819, 656, 504, 1080, 629, 440, 586, 457, 421, 712, 490, 508, 583, 420, 492, 617, 451, 414, 606, 540, 620, 478, 830, 543, 503, 956, 1392, 476, 412, 848, 461, 592, 651, 432, 819, 409, 604, 394, 635, 657, 448, 506, 431, 445, 493, 500, 723, 995, 383, 508, 383, 486, 765, 454, 631, 806, 487, 400, 473, 858, 476, 483, 1171, 479, 392, 745, 564, 437, 484, 560, 374, 949, 469, 708, 459, 527, 623, 1040, 650, 602, 475, 620]), Row(author='Shakespeare', vector=[620, 918, 389, 710, 339, 298, 282, 351, 408, 377, 309, 279, 465, 319, 409, 500, 510, 1135, 483, 559, 331, 654, 512, 268, 395, 292, 306, 467, 317, 355, 363, 317, 257, 439, 376, 459, 472, 375, 422, 393, 526, 385, 315, 542, 858, 315, 312, 622, 344, 390, 298, 397, 629, 257, 299, 293, 462, 453, 336, 414, 319, 370, 490, 417, 401, 755, 238, 361, 310, 382, 322, 322, 371, 518, 409, 312, 373, 602, 303, 289, 829, 332, 371, 469, 421, 344, 354, 399, 251, 710, 271, 307, 286, 279, 284, 642, 451, 431, 331, 568]), Row(author='Shakespeare', vector=[390, 954, 441, 691, 299, 309, 372, 287, 360, 331, 288, 281, 376, 267, 326, 458, 535, 982, 398, 523, 286, 690, 453, 254, 406, 269, 266, 503, 328, 369, 452, 226, 236, 409, 297, 377, 439, 389, 490, 271, 492, 398, 316, 558, 1001, 337, 241, 607, 316, 405, 319, 307, 599, 276, 298, 346, 415, 424, 279, 329, 323, 308, 316, 329, 354, 677, 281, 361, 258, 338, 357, 324, 339, 422, 320, 354, 331, 555, 293, 329, 859, 346, 345, 473, 378, 288, 308, 343, 272, 728, 316, 307, 263, 312, 273, 733, 404, 359, 289, 531]), Row(author='Shakespeare', vector=[2578, 3533, 1383, 3195, 1005, 1016, 1390, 1176, 1623, 1433, 1353, 1416, 1440, 1092, 1630, 1452, 1682, 4355, 1332, 1965, 1232, 2353, 1375, 1257, 1169, 1186, 1020, 2177, 1003, 1246, 2991, 1413, 1076, 1073, 1288, 1210, 1453, 1603, 863, 1205, 2156, 1461, 1185, 1241, 2895, 1842, 1120, 1887, 1063, 1435, 929, 1501, 2425, 1084, 1291, 1392, 1429, 1495, 995, 1024, 1105, 1055, 1320, 1036, 1345, 3267, 1038, 2263, 1429, 1200, 1695, 1002, 1230, 1835, 1128, 1047, 1096, 1768, 1100, 2300, 3523, 1269, 918, 2189, 1606, 1362, 1291, 1948, 878, 3392, 1229, 1177, 825, 1332, 1184, 1976, 1974, 1811, 1380, 1476]), Row(author='Shakespeare', vector=[836, 1320, 618, 1152, 381, 425, 570, 497, 559, 519, 580, 605, 644, 419, 597, 567, 888, 1263, 713, 855, 514, 987, 623, 422, 594, 435, 371, 789, 530, 548, 646, 358, 416, 575, 431, 690, 614, 598, 659, 457, 687, 512, 463, 987, 1623, 426, 387, 998, 504, 627, 422, 571, 826, 351, 454, 445, 598, 657, 404, 520, 471, 469, 402, 647, 546, 1101, 502, 622, 371, 419, 478, 492, 467, 697, 484, 444, 493, 1046, 423, 591, 1494, 429, 442, 836, 777, 463, 562, 616, 364, 1038, 481, 442, 402, 486, 383, 1121, 623, 655, 419, 872]), Row(author='Shakespeare', vector=[4003, 5661, 2040, 5045, 1910, 1374, 2259, 1901, 2708, 2270, 3176, 2179, 2175, 1635, 2365, 2537, 3127, 6227, 2186, 3073, 1854, 3702, 1909, 1918, 2439, 1991, 1695, 3566, 1536, 1760, 4725, 2385, 1793, 1896, 2232, 2261, 2531, 2429, 1396, 1985, 3404, 2633, 2029, 2256, 4586, 2350, 1709, 3165, 1608, 2337, 1363, 2641, 3883, 1633, 2006, 2020, 2348, 2817, 1393, 1726, 1836, 2605, 1778, 1763, 2432, 5719, 1589, 3702, 1592, 2051, 2714, 1809, 1806, 2809, 1896, 1636, 1920, 3229, 1922, 3401, 5630, 1972, 1568, 3427, 2411, 2114, 1941, 3044, 1559, 4836, 2076, 1848, 1308, 1828, 1921, 3483, 3211, 3439, 2084, 2473]), Row(author='Shakespeare', vector=[1333, 1622, 576, 1476, 527, 438, 494, 526, 618, 666, 526, 601, 750, 525, 651, 655, 819, 1808, 888, 806, 574, 1176, 752, 453, 681, 437, 489, 839, 554, 482, 573, 425, 545, 609, 550, 490, 647, 721, 550, 461, 838, 606, 540, 1251, 1510, 661, 469, 978, 545, 592, 538, 594, 995, 427, 607, 480, 700, 703, 467, 548, 509, 464, 627, 609, 620, 1241, 575, 572, 440, 481, 822, 499, 682, 783, 527, 483, 627, 967, 625, 529, 1447, 651, 489, 814, 1033, 559, 522, 584, 431, 1355, 561, 500, 552, 589, 469, 1273, 755, 715, 539, 783]), Row(author='Shakespeare', vector=[700, 1047, 374, 866, 337, 294, 454, 345, 394, 458, 314, 376, 398, 293, 566, 514, 619, 1023, 662, 509, 378, 859, 549, 327, 486, 329, 341, 573, 373, 349, 346, 304, 308, 446, 338, 345, 534, 500, 424, 336, 529, 530, 357, 528, 1192, 372, 338, 698, 360, 397, 364, 380, 525, 265, 377, 332, 498, 565, 284, 416, 371, 342, 465, 747, 418, 824, 272, 345, 294, 330, 446, 348, 393, 496, 298, 294, 399, 644, 706, 328, 897, 385, 331, 657, 425, 491, 382, 354, 288, 755, 383, 306, 319, 362, 288, 765, 509, 435, 431, 457]), Row(author='Shakespeare', vector=[964, 1071, 400, 1013, 486, 358, 512, 414, 536, 438, 527, 399, 455, 312, 485, 567, 633, 1147, 386, 739, 384, 832, 453, 396, 406, 377, 416, 744, 339, 364, 934, 406, 366, 501, 537, 416, 388, 496, 296, 404, 655, 504, 492, 653, 1516, 478, 296, 731, 364, 492, 283, 551, 855, 338, 473, 389, 620, 655, 298, 364, 295, 368, 382, 485, 551, 1266, 355, 543, 314, 464, 595, 423, 395, 586, 463, 309, 445, 708, 412, 584, 1225, 425, 424, 697, 488, 369, 430, 397, 319, 929, 420, 402, 371, 394, 398, 802, 629, 543, 454, 550]), Row(author='Shakespeare', vector=[5422, 8105, 2580, 7152, 2244, 1708, 3009, 2456, 3509, 3069, 3314, 2676, 2842, 2185, 3608, 3375, 4760, 8574, 2506, 4124, 2591, 4669, 2675, 2576, 2417, 2300, 2552, 4675, 2023, 2325, 6399, 3184, 2342, 2307, 2978, 2367, 3075, 3206, 1958, 2728, 4673, 2749, 2433, 2831, 5583, 2923, 2338, 4009, 2302, 2731, 1803, 3643, 4987, 2301, 2817, 2569, 3237, 3200, 1811, 2230, 2379, 2770, 2274, 2394, 2881, 7509, 2053, 4954, 2289, 2522, 3795, 2088, 2281, 3887, 2437, 2017, 2311, 3751, 2606, 4666, 7239, 2474, 2350, 5297, 3205, 2915, 2557, 3861, 1851, 7040, 2696, 3106, 1704, 2327, 2845, 4049, 4306, 4124, 2679, 3097]), Row(author='Shakespeare', vector=[580, 1080, 536, 919, 393, 316, 442, 394, 448, 422, 374, 330, 495, 395, 413, 510, 578, 1341, 507, 592, 416, 887, 479, 364, 426, 332, 316, 655, 362, 465, 439, 285, 344, 526, 356, 542, 510, 462, 504, 370, 598, 464, 397, 771, 1391, 424, 417, 791, 422, 504, 306, 476, 735, 360, 389, 366, 532, 690, 302, 337, 383, 406, 451, 447, 486, 892, 261, 416, 274, 390, 416, 349, 383, 602, 394, 396, 385, 721, 385, 388, 995, 348, 421, 641, 568, 457, 431, 426, 313, 998, 338, 425, 318, 356, 342, 911, 511, 496, 357, 567]), Row(author='Shakespeare', vector=[2710, 4300, 1509, 3297, 1088, 965, 1718, 1299, 1810, 1605, 1720, 1305, 1331, 1173, 1734, 1861, 2066, 4575, 1174, 2088, 1377, 2187, 1249, 1465, 1173, 1218, 1091, 2453, 1160, 1236, 2727, 1729, 1031, 1175, 1434, 1260, 1697, 1720, 961, 1335, 2384, 1379, 1392, 1192, 2599, 1498, 1004, 2126, 1578, 1533, 928, 1715, 2673, 1114, 1239, 1502, 1525, 1796, 886, 1467, 1221, 1310, 1172, 1238, 1703, 4133, 1086, 2531, 1290, 1250, 1905, 1009, 1121, 1941, 1192, 1586, 1170, 1869, 1349, 2265, 3885, 1225, 1306, 2074, 1602, 1550, 1226, 2441, 897, 3620, 1367, 1217, 1042, 1297, 1316, 2035, 2181, 2177, 1570, 1428]), Row(author='Shakespeare', vector=[1340, 1516, 551, 1110, 518, 448, 478, 542, 636, 612, 522, 559, 670, 510, 626, 722, 802, 1738, 965, 681, 669, 996, 655, 427, 551, 457, 527, 874, 540, 485, 631, 445, 467, 625, 578, 536, 705, 785, 611, 525, 789, 658, 547, 1261, 1496, 517, 531, 1014, 630, 564, 687, 707, 945, 586, 548, 474, 658, 713, 463, 549, 435, 623, 613, 512, 607, 1291, 557, 531, 451, 558, 686, 508, 530, 846, 505, 485, 548, 964, 635, 474, 1392, 516, 433, 787, 587, 721, 707, 664, 414, 1126, 489, 518, 502, 535, 464, 1198, 715, 761, 532, 793]), Row(author='Shakespeare', vector=[1000, 1251, 424, 1163, 551, 365, 556, 444, 503, 540, 497, 627, 568, 437, 503, 683, 660, 1366, 728, 752, 478, 909, 666, 357, 527, 469, 445, 768, 554, 455, 668, 424, 571, 618, 455, 441, 553, 569, 758, 444, 705, 588, 473, 795, 1466, 457, 465, 763, 483, 599, 549, 519, 848, 335, 612, 430, 677, 594, 394, 420, 421, 418, 499, 455, 598, 1057, 413, 575, 325, 412, 664, 406, 465, 762, 449, 395, 649, 821, 734, 529, 1171, 434, 516, 748, 500, 485, 456, 455, 403, 1175, 432, 506, 328, 498, 453, 1052, 712, 515, 521, 763]), Row(author='Shakespeare', vector=[999, 1251, 424, 1163, 551, 365, 556, 444, 503, 540, 497, 627, 568, 437, 503, 683, 660, 1366, 728, 752, 478, 909, 666, 357, 527, 469, 445, 768, 554, 455, 668, 424, 570, 618, 455, 441, 553, 569, 758, 444, 705, 588, 473, 795, 1466, 457, 465, 763, 483, 599, 549, 519, 848, 335, 612, 430, 677, 594, 394, 420, 421, 418, 499, 455, 598, 1057, 413, 575, 325, 412, 664, 406, 465, 762, 449, 395, 649, 821, 734, 529, 1171, 434, 516, 748, 500, 485, 456, 455, 403, 1175, 432, 506, 328, 498, 453, 1052, 712, 515, 521, 763]), Row(author='Shakespeare', vector=[4042, 5426, 2028, 5181, 1897, 1393, 2224, 1863, 2663, 2241, 2296, 1910, 2213, 1688, 2571, 2624, 2890, 6294, 1853, 3110, 2024, 3507, 1854, 1801, 1807, 1846, 1675, 3678, 1529, 1815, 5072, 2053, 1812, 1819, 2319, 1873, 2123, 3191, 1441, 2163, 3425, 2311, 1918, 2196, 4441, 2137, 1671, 2911, 1955, 2188, 1540, 2562, 4016, 1598, 1958, 2052, 3006, 2469, 1395, 1859, 1821, 1722, 1816, 1808, 2337, 5733, 1639, 3710, 1645, 1953, 2842, 1794, 1812, 2904, 1744, 1686, 1832, 2947, 1808, 3269, 5621, 1881, 1591, 3360, 2296, 2346, 1781, 2719, 1406, 5273, 1949, 2152, 1432, 1858, 1962, 3367, 3171, 3114, 2083, 2340]), Row(author='Shakespeare', vector=[912, 1076, 462, 943, 558, 409, 407, 397, 492, 719, 1337, 391, 453, 444, 410, 604, 820, 1256, 614, 639, 423, 846, 656, 327, 562, 436, 433, 699, 502, 466, 686, 387, 430, 472, 413, 420, 528, 551, 541, 429, 696, 557, 558, 672, 1221, 560, 420, 824, 545, 444, 648, 494, 795, 343, 535, 407, 673, 538, 403, 349, 515, 381, 454, 491, 463, 1045, 347, 467, 320, 371, 534, 382, 481, 736, 470, 380, 456, 648, 444, 427, 1067, 480, 551, 617, 497, 478, 667, 504, 412, 973, 469, 436, 352, 656, 402, 797, 578, 489, 507, 635])]\n"
     ]
    }
   ],
   "source": [
    "austen = ['senseandsensibility.txt','mansfield_park.txt','emma.txt','persuasion.txt','northanger_abbey.txt','lady_susan.txt', \n",
    "    'prideandpredjudice.txt']\n",
    "#austen = ['hdfs://saltdean/data/library/'+s for s in austen] \n",
    "austen = ['../../Data/library/'+s for s in austen] \n",
    "#print(austen)\n",
    "\n",
    "av_RDD = f_ngVec_RDD.map(lambda f_wVec: ('Austen' if (f_wVec[0] in austen) else 'Shakespeare',f_wVec[1])) \n",
    "\n",
    "from pyspark.sql import Row\n",
    "    \n",
    "row_RDD = av_RDD.map(lambda av: Row(author=av[0], vector=av[1])) \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Create a dataFrame from the RDD\n",
    "library_DF = spark.createDataFrame(row_RDD)\n",
    "library_DF.createOrReplaceTempView(\"library\")\n",
    "print(\"library_DF.printSchema()\")\n",
    "library_DF.printSchema()\n",
    "print(\"library_DF.show()\")\n",
    "library_DF.show()\n",
    "#print(\"library_DF.describe()\")\n",
    "#library_DF.describe()\n",
    "\n",
    "# SQL can be used over DataFrames that have been registered as a table.\n",
    "SQL1 = \"SELECT author,vector FROM library WHERE author=='Austen'\"\n",
    "austen_vectors = spark.sql(SQL1)\n",
    "print(SQL1)\n",
    "austen_vectors.show()\n",
    "\n",
    "SQL2 = \"SELECT author,vector FROM library WHERE author!='Austen'\"\n",
    "other_vectors = spark.sql(SQL2)\n",
    "print(SQL2)\n",
    "print(other_vectors.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make DataFrames from the newsgroups dataset\n",
    "\n",
    "To use the newsgroups dataset we need to parse the messages by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_dataFrame started\n",
      "325091\n",
      "fng2_RDD.count() 325091\n",
      "make_dataFrame started\n",
      "325091\n",
      "fng2_RDD.count() 325091\n",
      "2000\n",
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      "\n",
      "+--------------------+-----+------+\n",
      "|            features|label|weight|\n",
      "+--------------------+-----+------+\n",
      "|[32.0,67.0,19.0,4...|  0.0|   1.0|\n",
      "|[127.0,105.0,28.0...|  0.0|   1.0|\n",
      "|[12.0,27.0,8.0,18...|  0.0|   1.0|\n",
      "|[10.0,8.0,1.0,17....|  0.0|   1.0|\n",
      "|[1.0,1.0,0.0,3.0,...|  0.0|   1.0|\n",
      "+--------------------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "#dirPath1 = '/data/tempstore/newsgroups/alt.atheism'\n",
    "#dirPath2 = '/data/tempstore/newsgroups/comp.graphics'\n",
    "\n",
    "dirPath1 = '../../Data/20_newsgroups/alt.atheism'\n",
    "dirPath2 = '../../Data/20_newsgroups/comp.graphics'\n",
    "\n",
    "\n",
    "N  = 100 # vector size\n",
    "NG =  1 # max n-gram size\n",
    "\n",
    "# remove the headers, get the sender and the main text\n",
    "def parseMessage(ft): \n",
    "    fn,text = ft # unpack the filename and text content \n",
    "    # Use a regular expression to match the text\n",
    "    matchObj = re.search(r'.+^(Lines:|NNTP-Posting-Host:) (.*)', text,re.DOTALL|re.MULTILINE) \n",
    "    if(matchObj): # only if the pattern has matched \n",
    "        text = matchObj.group(2) # can we replace the text \n",
    "    else:\n",
    "        return None # otherwise we don't return anything\n",
    "    return (fn,text)\n",
    "\n",
    "# we need a SparkSession to create DataFrames\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# we need to create our feature vecotrs using the pyspark.ml.linalg.Vectors class,\n",
    "# in order to use the CrossValidation later.\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Make a DataFrame with labels and n-Gram vectors \n",
    "def make_dataFrame(dirPath, argLabel):\n",
    "    print(\"make_dataFrame started\")\n",
    "    ft_RDD = sc.wholeTextFiles(dirPath) # add code to create an RDD with wholeTextFiles\n",
    "    ft2_RDD = ft_RDD.map(parseMessage) # get (file,text) \n",
    "    #print(\"ft2_RDD.take(2)\", ft2_RDD.take(2))\n",
    "    fng_RDD = ft2_RDD.flatMap(partial(splitFileNGrams,n=NG)) # \n",
    "    #print(\"fng_RDD.take(2)\", fng_RDD.take(2))\n",
    "    print(fng_RDD.count())\n",
    "    fng2_RDD = fng_RDD.filter(lambda x: x is not None)\n",
    "    print(\"fng2_RDD.count()\", fng2_RDD.count())\n",
    "    fng_1_RDD = fng2_RDD.map(lambda x: (x,1))  # change (fs,ng) to ((fs,ng),1) - fs is actually a tuple, but we ignore that here\n",
    "    fng_c_RDD = fng_1_RDD.reduceByKey(add) #as above\n",
    "    f_ngcL_RDD = fng_c_RDD.map(reGrpLst) #as above\n",
    "    f_ngcL2_RDD = f_ngcL_RDD.reduceByKey(add) #<<< create [(w,c), ... ,(w,c)] lists per file \n",
    "    f_ngVec_RDD = f_ngcL2_RDD.map(lambda f_wc: (f_wc[0],hashing_vectorizer(f_wc[1],N)))\n",
    "    # create Row objects with Vectors, as required by the algorithms in the 'ml' package.\n",
    "    rows_RDD = f_ngVec_RDD.map(lambda f_ngVec: Row( features=Vectors.dense(f_ngVec[1]), weight=1.0, label=float(argLabel)))\n",
    "    rows_DF = spark.createDataFrame(rows_RDD) # create a DataFrame\n",
    "    return rows_DF\n",
    "\n",
    "# for testing the parseMessages function\n",
    "#ft_RDD = sc.wholeTextFiles(dirPath1) # create an RDD with wholeTextFiles    \n",
    "#txts = ft_RDD.take(3) # take into a local list\n",
    "#txts2 = list(map(parseMessage,txts))# and apply removeHeader (NOTE: this is different from an RDD map!)\n",
    "#print(txts2)\n",
    "\n",
    "rows1_DF = make_dataFrame(dirPath1, 0)\n",
    "rows2_DF = make_dataFrame(dirPath1, 1)\n",
    "rows_DF = rows1_DF.union(rows2_DF)\n",
    "rows_DF.createOrReplaceTempView(\"newsgroups\")\n",
    "print(rows_DF.count())\n",
    "rows_DF.printSchema()\n",
    "\n",
    "rows_DF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Use the spark.ml cross validator\n",
    "\n",
    "Now we can use the CrossValidator, which comes with the ML module. We only need to set up the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n",
      "[5.48832004997]\n",
      "-2.6357106786866553\n",
      "+---------+-----+--------------------+--------------------+----------+\n",
      "| features|label|       rawPrediction|         probability|prediction|\n",
      "+---------+-----+--------------------+--------------------+----------+\n",
      "|    [1.0]|  1.0|[-2.8526093712877...|[0.05454659212542...|       1.0|\n",
      "|(1,[],[])|  0.0|[2.63571067868665...|[0.93312479504475...|       0.0|\n",
      "+---------+-----+--------------------+--------------------+----------+\n",
      "\n",
      "None\n",
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      "\n",
      "(100,[],[])\n",
      "0.0\n",
      "+--------------------+-----+------+-------------+-----------+----------+\n",
      "|            features|label|weight|rawPrediction|probability|prediction|\n",
      "+--------------------+-----+------+-------------+-----------+----------+\n",
      "|[32.0,67.0,19.0,4...|  1.0|   1.0|   [-0.0,0.0]|  [0.5,0.5]|       0.0|\n",
      "|[127.0,105.0,28.0...|  1.0|   1.0|   [-0.0,0.0]|  [0.5,0.5]|       0.0|\n",
      "|[12.0,27.0,8.0,18...|  1.0|   1.0|   [-0.0,0.0]|  [0.5,0.5]|       0.0|\n",
      "|[10.0,8.0,1.0,17....|  1.0|   1.0|   [-0.0,0.0]|  [0.5,0.5]|       0.0|\n",
      "|[1.0,1.0,0.0,3.0,...|  1.0|   1.0|   [-0.0,0.0]|  [0.5,0.5]|       0.0|\n",
      "+--------------------+-----+------+-------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "starting cross-validation\n",
      "finished cross-validation\n",
      "cvModel.bestModel NaiveBayes_4ad193959cfeab8e0be6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5000000000000001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to import the cliassifers from the ML package now.\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# The CrossValidator and ParamGridBuilder enable the automatic tuning\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "# The evaluator tests the model\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "df = sc.parallelize([\n",
    "     Row(label=1.0, features=Vectors.dense(1.0)),\n",
    "     Row(label=0.0, features=Vectors.sparse(1, [], []))]).toDF()\n",
    "lr = LogisticRegression(maxIter=5, regParam=0.01)\n",
    "df.printSchema()\n",
    "model = lr.fit(df)\n",
    "print(model.coefficients)\n",
    "print(model.intercept)\n",
    "results = model.transform( df)\n",
    "print(results.show(3))\n",
    "\n",
    "\n",
    "#df = spark.read.format(\"libsvm\").load('sample_libsvm_data.txt')\n",
    "#print('df.count()',df.count())\n",
    "\n",
    "# WARNING the classifiers in ML seem to have a problem with out spam dataset \n",
    "# (at least when it has a non-trivial size). \n",
    "# I am investigating, the reason is not known at the moment. \n",
    "r1 = rows1_DF.head(50)\n",
    "r2 = rows2_DF.head(50)\n",
    "df = sc.parallelize(r1+r2).toDF()\n",
    "\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.0)\n",
    "df.printSchema()\n",
    "model = lr.fit(df)\n",
    "print(model.coefficients)\n",
    "print(model.intercept)\n",
    "results = model.transform(df) \n",
    "results = results.filter(results.label==1)\n",
    "print(results.show(5))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "#lr = LogisticRegression(maxIter=20, regParam=0.0)\n",
    "#grid = ParamGridBuilder().addGrid(lr.maxIter, [3, 10, 30, 100]).addGrid(lr.regParam, [0, 0.03, 30, 100]).build()\n",
    "nb = NaiveBayes()\n",
    "grid = ParamGridBuilder().addGrid(nb.smoothing, [0.03, 0.1, 0.3, 1, 3, 10]).build()\n",
    "print(\"starting cross-validation\")\n",
    "#cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "cv = CrossValidator(estimator=nb, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "cvModel = cv.fit(df)\n",
    "print(\"finished cross-validation\")\n",
    "print(\"cvModel.bestModel\", cvModel.bestModel)\n",
    "evaluator.evaluate(cvModel.transform(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
