{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ALS with Spark, evaluating CV results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data, split into tokens and create a structured DataFrame. For low level tasks like splitting strings, we need to use an RDD, where we can apply a `map` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(movieId=2, rating=3.0, timestamp=1424380312, userId=0)]\n",
      "[Row(movieId=2, rating=3.0, timestamp=1424380312, userId=0)]\n",
      "DataFrame[movieId: bigint, rating: double, timestamp: bigint, userId: bigint]\n",
      "DataFrame[movieId: bigint, rating: double, timestamp: bigint, userId: bigint]\n",
      "DataFrame[summary: string, movieId: string, rating: string, timestamp: string, userId: string]\n",
      "1194\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "# the imports are used creating the data frame\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate() # create a SparkSession \n",
    "\n",
    "# this gets us an RDD. (could also be done with RDD.textFile in this case)\n",
    "lines = spark.read.text(\"hdfs://saltdean/data/movielens/sample_movielens_ratings.txt\").rdd \n",
    "# now split the lines at the '::'\n",
    "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                    rating=float(p[2]), timestamp=int(p[3])))\n",
    "print(ratingsRDD.take(1))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "print(ratings.take(1))\n",
    "print(ratings)\n",
    "\n",
    "ratings.createOrReplaceTempView('ratings') # register the DataFrame so that we can use it with Spark SQL.\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2]) # split into test and training set\n",
    "print(training) # just for testing, should show the four columns\n",
    "print(training.describe()) # just for testing, should show the four columns\n",
    "print(training.count()) # just for testing, should be around 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a very simple estimate as the baseline: calculate the mean of all ratings.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanRating 1.7741505662891406\n",
      "se_df DataFrame[se: double]\n",
      "meanSE 1.2883502813858931\n"
     ]
    }
   ],
   "source": [
    "SQL1 = 'SELECT AVG(rating) FROM ratings'\n",
    "row = spark.sql(SQL1).collect()[0] # get the single row with the result\n",
    "\n",
    "meanRating = row['avg(rating)'] # access Row as a map \n",
    "print('meanRating',meanRating)\n",
    "\n",
    "se_rdd = test.rdd.map(lambda row: Row(se = pow(row['rating']-meanRating,2)) ) \n",
    "se_df = spark.createDataFrame(se_rdd) \n",
    "se_df.createOrReplaceTempView('se')\n",
    "print('se_df',se_df)\n",
    "SQL2 = 'SELECT AVG(se) FROM se'\n",
    "row = spark.sql(SQL2).collect()[0]\n",
    "meanSE = row['avg(se)'] # access Row as a map \n",
    "print('meanSE',meanSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an ALS estimator and a parameter grid to explore different values for the `rank` and `regParam` parameter of the ALS. Then build a cross-validator to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting cross-validation\n",
      "finished cross-validation\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, rank=5, regParam=0.1, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(als.regParam, [0.03,0.1,0.3,1,3]) \\\n",
    "    .addGrid(als.rank, [1,3,10,30,100]).build()\n",
    "\n",
    "regEval = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "nFolds = 6 # reh\n",
    "\n",
    "crossVal = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=regEval, numFolds=nFolds)\n",
    "print('starting cross-validation')\n",
    "cvModel = crossVal.fit(training)\n",
    "print('finished cross-validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning and CV\n",
    "\n",
    "Take the trained cvModel and extract the best parameter values by inspecting the estimatorParameterMap. Compare the RMSE value to that of the mean for different parameter settings.\n",
    "\n",
    "The parameter maps and metrics lists we get from the `cvModel` are local Python list, so we need to use local methods, not RDD methods. There are however similar functions available, in particular `map` and `zip`, which work like for RDDs and `list` which is similar to RDD.collect in creating a mapped list. See here for documentation:  [https://docs.python.org/3/library/functions.html](https://docs.python.org/3/library/functions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.541827322676414, 7.504204833346607, 7.543234042274206, 9.151977064097533, 12.854056304461999, 8.160033474392774, 7.375578896757023, 7.043424389032287, 9.152147888376694, 12.854052608775902, 9.671700080379987, 6.889642010167341, 6.7899610659035075, 9.152071560984172, 12.854057648205941, 8.237358092183237, 6.551492911565812, 6.793163339438315, 9.15208245573481, 12.854055597319896, 8.286559416484007, 6.609700573830148, 6.787986162609502, 9.152061545631911, 12.854051124520904]\n",
      "[{Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 1, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.03}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 1, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.1}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 1, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.3}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 1, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 1}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 1, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 3}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 3, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.03}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 3, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.1}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 3, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.3}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 3, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 1}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 3, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 3}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 10, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.03}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 10, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.1}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 10, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.3}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 10, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 1}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 10, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 3}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 30, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.03}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 30, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.1}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 30, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.3}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 30, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 1}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 30, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 3}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 100, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.03}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 100, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.1}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 100, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.3}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 100, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 1}, {Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 100, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 3}]\n",
      "({Param(parent='ALS_417d92275eb0e2db2b0c', name='rank', doc='rank of the factorization'): 30, Param(parent='ALS_417d92275eb0e2db2b0c', name='regParam', doc='regularization parameter (>= 0).'): 0.1}, 1.0919154852609687)\n",
      "Root-mean-square error = 0.9782916021446896\n"
     ]
    }
   ],
   "source": [
    "print(cvModel.avgMetrics) # the metrics form the CrossValidation\n",
    "print(cvModel.getEstimatorParamMaps())\n",
    "# use Python zip and list (not RDD functions, these are local Python object) to create a joint paramter and result list\n",
    "paramMap = list(zip(cvModel.getEstimatorParamMaps(),cvModel.avgMetrics)) \n",
    "# use Python map to create a joint paramter and result list\n",
    "paramMap = list(map(lambda epm_am: (epm_am[0],epm_am[1]/nFolds), paramMap)) \n",
    "#print(paramMap)\n",
    "# use Python min to get the best params (i.e. those producing minimal RMSE) \n",
    "paramMax = min(paramMap, key=lambda x: x[1])\n",
    "print(paramMax)\n",
    "\n",
    "# Evaluate the cvModel by computing the RMSE on the test data\n",
    "predictions = cvModel.transform(test)\n",
    "rmse = regEval.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('userId', 'int'),\n",
       " ('movieId', 'int'),\n",
       " ('rating', 'double'),\n",
       " ('timestamp', 'int')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = spark.read.csv('hdfs://saltdean/data/movielens/ml-latest-small/ratings.csv',header=True,inferSchema=True) \n",
    "ratings.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
